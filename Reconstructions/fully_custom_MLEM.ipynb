{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c93cf94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Shape (pairs, coords) : (62660, 6)\n",
      "\n",
      "Filtered shape: (6591, 6)\n",
      "\n",
      "x range: min=-278.1294250488281, max=278.1666564941406\n",
      "y range: min=-278.4194641113281, max=277.8843688964844\n",
      "z range: min=-147.99453735351562, max=147.9492950439453\n",
      "Voxel grid: [278, 276, 149] voxels\n",
      "Grid origin: [-275.20611572265625, -275.9847412109375, -148.61593627929688] mm\n",
      "Voxel size: 2.0 mm\n",
      "Starting MLEM reconstruction...\n",
      "Building system matrix...\n",
      "Processed batch 10/10\n",
      "System matrix: torch.Size([100, 11432472]), 16938 non-zero elements\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from typing import Tuple, Optional\n",
    "import math\n",
    "\n",
    "class CUDAMLEMReconstructor:\n",
    "    \"\"\"\n",
    "    Maximum Likelihood Expectation Maximization for PET reconstruction\n",
    "    with scatter, randoms, and attenuation corrections using CUDA acceleration.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lors: torch.Tensor, voxel_size: float = 2.0, device: str = 'cuda'):\n",
    "        \"\"\"\n",
    "        Initialize MLEM reconstructor.\n",
    "        \n",
    "        Args:\n",
    "            lors: Tensor of shape (num_lors, 6) with [x1,y1,z1,x2,y2,z2] endpoints in mm\n",
    "            voxel_size: Voxel size in mm\n",
    "            device: 'cuda' or 'cpu'\n",
    "        \"\"\"\n",
    "        self.device = torch.device(device)\n",
    "        self.lors = lors.to(self.device).float()\n",
    "        self.voxel_size = voxel_size\n",
    "        self.num_lors = lors.shape[0]\n",
    "        \n",
    "        # Calculate voxel space bounds\n",
    "        self._calculate_voxel_space()\n",
    "        \n",
    "        # Initialize system matrix storage\n",
    "        self.system_matrix = None\n",
    "        self.sensitivity_image = None\n",
    "        \n",
    "    def _calculate_voxel_space(self):\n",
    "        \"\"\"Calculate voxel grid dimensions from LOR coordinates.\"\"\"\n",
    "        # Get coordinate bounds with small padding\n",
    "        coords = self.lors.view(-1, 3)  # Reshape to (2*num_lors, 3)\n",
    "        min_coords = coords.min(dim=0)[0] - self.voxel_size\n",
    "        max_coords = coords.max(dim=0)[0] + self.voxel_size\n",
    "        \n",
    "        # Calculate grid dimensions\n",
    "        self.grid_size = ((max_coords - min_coords) / self.voxel_size).ceil().int()\n",
    "        self.grid_origin = min_coords\n",
    "        \n",
    "        print(f\"Voxel grid: {self.grid_size.tolist()} voxels\")\n",
    "        print(f\"Grid origin: {self.grid_origin.tolist()} mm\")\n",
    "        print(f\"Voxel size: {self.voxel_size} mm\")\n",
    "        \n",
    "    def _siddon_ray_trace(self, lors_batch: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        GPU-accelerated Siddon ray tracing for LOR-voxel intersections.\n",
    "        \n",
    "        Returns:\n",
    "            voxel_indices: Flattened voxel indices\n",
    "            lor_indices: Corresponding LOR indices  \n",
    "            weights: Intersection lengths\n",
    "        \"\"\"\n",
    "        batch_size = lors_batch.shape[0]\n",
    "        \n",
    "        # Convert LOR endpoints to voxel coordinates\n",
    "        p1 = (lors_batch[:, :3] - self.grid_origin) / self.voxel_size\n",
    "        p2 = (lors_batch[:, 3:] - self.grid_origin) / self.voxel_size\n",
    "        \n",
    "        # Ray direction and length\n",
    "        ray_dir = p2 - p1\n",
    "        ray_length = torch.norm(ray_dir, dim=1, keepdim=True)\n",
    "        ray_dir = ray_dir / (ray_length + 1e-10)\n",
    "        \n",
    "        # Initialize storage for intersections\n",
    "        max_intersections = int(torch.max(self.grid_size).item()) * 2\n",
    "        voxel_indices = []\n",
    "        lor_indices = []\n",
    "        weights = []\n",
    "        \n",
    "        # Process each LOR\n",
    "        for i in range(batch_size):\n",
    "            voxels, lens = self._trace_single_ray(p1[i], p2[i], ray_dir[i])\n",
    "            if len(voxels) > 0:\n",
    "                voxel_indices.extend(voxels)\n",
    "                lor_indices.extend([i] * len(voxels))\n",
    "                weights.extend(lens)\n",
    "        \n",
    "        if len(voxel_indices) == 0:\n",
    "            return torch.empty(0, dtype=torch.long, device=self.device), \\\n",
    "                   torch.empty(0, dtype=torch.long, device=self.device), \\\n",
    "                   torch.empty(0, dtype=torch.float, device=self.device)\n",
    "        \n",
    "        return torch.tensor(voxel_indices, device=self.device), \\\n",
    "               torch.tensor(lor_indices, device=self.device), \\\n",
    "               torch.tensor(weights, device=self.device) * self.voxel_size\n",
    "    \n",
    "    def _trace_single_ray(self, p1: torch.Tensor, p2: torch.Tensor, ray_dir: torch.Tensor):\n",
    "        \"\"\"Trace a single ray through the voxel grid using Siddon's algorithm.\"\"\"\n",
    "        voxels = []\n",
    "        lengths = []\n",
    "        \n",
    "        # Current position\n",
    "        pos = p1.clone()\n",
    "        end_pos = p2\n",
    "        \n",
    "        # Step sizes for each dimension\n",
    "        step = torch.sign(ray_dir)\n",
    "        delta = torch.abs(1.0 / (ray_dir + 1e-10))\n",
    "        \n",
    "        # Current voxel\n",
    "        current_voxel = pos.floor().int()\n",
    "        \n",
    "        # Distance to next voxel boundary\n",
    "        if ray_dir[0] > 0:\n",
    "            next_x = (current_voxel[0] + 1).float()\n",
    "        else:\n",
    "            next_x = current_voxel[0].float()\n",
    "        \n",
    "        if ray_dir[1] > 0:\n",
    "            next_y = (current_voxel[1] + 1).float()\n",
    "        else:\n",
    "            next_y = current_voxel[1].float()\n",
    "            \n",
    "        if ray_dir[2] > 0:\n",
    "            next_z = (current_voxel[2] + 1).float()\n",
    "        else:\n",
    "            next_z = current_voxel[2].float()\n",
    "        \n",
    "        t_max = torch.tensor([\n",
    "            abs((next_x - pos[0]) / (ray_dir[0] + 1e-10)),\n",
    "            abs((next_y - pos[1]) / (ray_dir[1] + 1e-10)), \n",
    "            abs((next_z - pos[2]) / (ray_dir[2] + 1e-10))\n",
    "        ], device=self.device)\n",
    "        \n",
    "        t_current = 0.0\n",
    "        t_end = torch.norm(end_pos - pos).item()\n",
    "        \n",
    "        # Traverse voxels\n",
    "        while t_current < t_end:\n",
    "            # Check if current voxel is valid\n",
    "            if (current_voxel >= 0).all() and (current_voxel < self.grid_size).all():\n",
    "                # Calculate intersection length\n",
    "                t_next = min(t_max.min().item(), t_end)\n",
    "                length = t_next - t_current\n",
    "                \n",
    "                if length > 1e-6:\n",
    "                    voxel_idx = (current_voxel[0] * self.grid_size[1] * self.grid_size[2] + \n",
    "                               current_voxel[1] * self.grid_size[2] + current_voxel[2]).item()\n",
    "                    voxels.append(voxel_idx)\n",
    "                    lengths.append(length)\n",
    "            \n",
    "            # Move to next voxel\n",
    "            if t_max.min() >= t_end:\n",
    "                break\n",
    "                \n",
    "            min_dim = t_max.argmin()\n",
    "            t_current = t_max[min_dim].item()\n",
    "            current_voxel[min_dim] += step[min_dim].int()\n",
    "            t_max[min_dim] += delta[min_dim]\n",
    "        \n",
    "        return voxels, lengths\n",
    "    \n",
    "    def build_system_matrix(self, batch_size: int = 10):\n",
    "        \"\"\"Build sparse system matrix using batched ray tracing.\"\"\"\n",
    "        print(\"Building system matrix...\")\n",
    "        \n",
    "        all_voxel_indices = []\n",
    "        all_lor_indices = []\n",
    "        all_weights = []\n",
    "        \n",
    "        # Process LORs in batches to manage memory\n",
    "        num_batches = (self.num_lors + batch_size - 1) // batch_size\n",
    "        \n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min((batch_idx + 1) * batch_size, self.num_lors)\n",
    "            \n",
    "            lors_batch = self.lors[start_idx:end_idx]\n",
    "            voxel_idx, lor_idx, weights = self._siddon_ray_trace(lors_batch)\n",
    "            \n",
    "            # Adjust LOR indices for global indexing\n",
    "            lor_idx += start_idx\n",
    "            \n",
    "            all_voxel_indices.append(voxel_idx)\n",
    "            all_lor_indices.append(lor_idx)\n",
    "            all_weights.append(weights)\n",
    "            \n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f\"Processed batch {batch_idx + 1}/{num_batches}\")\n",
    "        \n",
    "        # Concatenate all results\n",
    "        voxel_indices = torch.cat(all_voxel_indices)\n",
    "        lor_indices = torch.cat(all_lor_indices)\n",
    "        weights = torch.cat(all_weights)\n",
    "        \n",
    "        # Create sparse system matrix\n",
    "        total_voxels = self.grid_size.prod().item()\n",
    "        indices = torch.stack([lor_indices, voxel_indices])\n",
    "        \n",
    "        self.system_matrix = torch.sparse_coo_tensor(\n",
    "            indices, weights, (self.num_lors, total_voxels), device=self.device\n",
    "        ).coalesce()\n",
    "        \n",
    "        print(f\"System matrix: {self.system_matrix.shape}, {len(weights)} non-zero elements\")\n",
    "        \n",
    "        # Calculate sensitivity image (column sums)\n",
    "        self.sensitivity_image = torch.sparse.sum(self.system_matrix, dim=0).to_dense()\n",
    "        self.sensitivity_image[self.sensitivity_image == 0] = 1.0  # Avoid division by zero\n",
    "    \n",
    "    def estimate_scatter(self, image: torch.Tensor, scatter_fraction: float = 0.15) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Simple scatter estimation using convolution-based approach.\n",
    "        More sophisticated scatter models (Monte Carlo, single scatter simulation) \n",
    "        could be implemented here.\n",
    "        \"\"\"\n",
    "        # Reshape image to 3D for convolution\n",
    "        img_3d = image.view(self.grid_size[0], self.grid_size[1], self.grid_size[2]).unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Simple Gaussian scatter kernel (approximation)\n",
    "        kernel_size = 7\n",
    "        sigma = 2.0\n",
    "        kernel = torch.zeros(1, 1, kernel_size, kernel_size, kernel_size, device=self.device)\n",
    "        \n",
    "        # Create 3D Gaussian kernel\n",
    "        center = kernel_size // 2\n",
    "        for i in range(kernel_size):\n",
    "            for j in range(kernel_size):\n",
    "                for k in range(kernel_size):\n",
    "                    dist_sq = (i - center)**2 + (j - center)**2 + (k - center)**2\n",
    "                    kernel[0, 0, i, j, k] = math.exp(-dist_sq / (2 * sigma**2))\n",
    "        \n",
    "        kernel = kernel / kernel.sum()\n",
    "        \n",
    "        # Apply convolution for scatter estimate\n",
    "        scattered = F.conv3d(img_3d, kernel, padding=kernel_size//2)\n",
    "        scattered = scattered.squeeze().flatten()\n",
    "        \n",
    "        # Forward project scattered activity\n",
    "        scatter_sino = torch.sparse.mm(self.system_matrix, scattered.unsqueeze(1)).squeeze()\n",
    "        \n",
    "        return scatter_sino * scatter_fraction\n",
    "    \n",
    "    def estimate_randoms(self, singles_rate: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Estimate random coincidences. \n",
    "        For simplicity, using uniform randoms. In practice, this would use\n",
    "        singles rates and detector geometry.\n",
    "        \"\"\"\n",
    "        if singles_rate is None:\n",
    "            # Simple uniform randoms estimate\n",
    "            randoms_rate = torch.full((self.num_lors,), 0.05, device=self.device)\n",
    "        else:\n",
    "            # More sophisticated randoms calculation could be implemented\n",
    "            randoms_rate = singles_rate * 0.01  # Simplified calculation\n",
    "        \n",
    "        return randoms_rate\n",
    "    \n",
    "    def apply_attenuation(self, sino: torch.Tensor, attenuation_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply attenuation correction using provided attenuation mask.\n",
    "        \n",
    "        Args:\n",
    "            sino: Sinogram data\n",
    "            attenuation_mask: 3D attenuation factors, same shape as voxel grid\n",
    "        \"\"\"\n",
    "        # Forward project attenuation mask to get LOR attenuation factors\n",
    "        atten_flat = attenuation_mask.flatten()\n",
    "        atten_factors = torch.sparse.mm(self.system_matrix, atten_flat.unsqueeze(1)).squeeze()\n",
    "        \n",
    "        # Apply exponential attenuation\n",
    "        atten_correction = torch.exp(-atten_factors)\n",
    "        return sino * atten_correction\n",
    "    \n",
    "    def forward_project(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward project image to sinogram space.\"\"\"\n",
    "        return torch.sparse.mm(self.system_matrix, image.unsqueeze(1)).squeeze()\n",
    "    \n",
    "    def back_project(self, sino: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Back project sinogram to image space.\"\"\"\n",
    "        return torch.sparse.mm(self.system_matrix.t(), sino.unsqueeze(1)).squeeze()\n",
    "    \n",
    "    def reconstruct(self, measured_data: torch.Tensor, attenuation_mask: torch.Tensor,\n",
    "                   num_iterations: int = 20, convergence_threshold: float = 1e-4) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform MLEM reconstruction with scatter, randoms, and attenuation corrections.\n",
    "        \n",
    "        Args:\n",
    "            measured_data: Measured sinogram data (num_lors,)\n",
    "            attenuation_mask: 3D attenuation correction factors\n",
    "            num_iterations: Maximum number of iterations\n",
    "            convergence_threshold: Relative change threshold for convergence\n",
    "            \n",
    "        Returns:\n",
    "            Reconstructed image as 1D tensor\n",
    "        \"\"\"\n",
    "        print(\"Starting MLEM reconstruction...\")\n",
    "        \n",
    "        # Build system matrix if not already done\n",
    "        if self.system_matrix is None:\n",
    "            self.build_system_matrix()\n",
    "        \n",
    "        # Initialize image with uniform activity\n",
    "        total_voxels = self.grid_size.prod().item()\n",
    "        image = torch.ones(total_voxels, device=self.device)\n",
    "        \n",
    "        # Apply attenuation to measured data\n",
    "        measured_data = self.apply_attenuation(measured_data.to(self.device), \n",
    "                                             attenuation_mask.to(self.device))\n",
    "        \n",
    "        prev_likelihood = float('inf')\n",
    "        \n",
    "        for iteration in range(num_iterations):\n",
    "            # Forward project current estimate\n",
    "            estimated_sino = self.forward_project(image)\n",
    "            \n",
    "            # Add scatter and randoms\n",
    "            scatter = self.estimate_scatter(image)\n",
    "            randoms = self.estimate_randoms()\n",
    "            \n",
    "            total_estimated = estimated_sino + scatter + randoms\n",
    "            total_estimated = torch.clamp(total_estimated, min=1e-10)  # Avoid division by zero\n",
    "            \n",
    "            # Calculate likelihood ratio\n",
    "            ratio = measured_data / total_estimated\n",
    "            \n",
    "            # Back project ratio\n",
    "            correction = self.back_project(ratio)\n",
    "            \n",
    "            # MLEM update\n",
    "            image = image * correction / self.sensitivity_image\n",
    "            image = torch.clamp(image, min=0)  # Enforce non-negativity\n",
    "            \n",
    "            # Calculate log-likelihood for convergence check\n",
    "            likelihood = torch.sum(measured_data * torch.log(total_estimated) - total_estimated).item()\n",
    "            \n",
    "            # Check convergence\n",
    "            if iteration > 0:\n",
    "                rel_change = abs(likelihood - prev_likelihood) / abs(prev_likelihood)\n",
    "                print(f\"Iteration {iteration + 1}: Log-likelihood = {likelihood:.2f}, \"\n",
    "                      f\"Rel. change = {rel_change:.6f}\")\n",
    "                \n",
    "                if rel_change < convergence_threshold:\n",
    "                    print(f\"Converged after {iteration + 1} iterations\")\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"Iteration {iteration + 1}: Log-likelihood = {likelihood:.2f}\")\n",
    "            \n",
    "            prev_likelihood = likelihood\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def get_image_3d(self, image_1d: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Convert 1D image to 3D array.\"\"\"\n",
    "        return image_1d.view(self.grid_size[0], self.grid_size[1], self.grid_size[2])\n",
    "\n",
    "# Example usage\n",
    "def example_usage():\n",
    "    \"\"\"Example of how to use the MLEM reconstructor.\"\"\"\n",
    "    \n",
    "    # Generate example LOR data (normally this would be real PET data)\n",
    "    num_lors = 500\n",
    "    lors = torch.randn(num_lors, 6) * 100  # Random LORs in mm\n",
    "    \n",
    "    # Generate example measured data\n",
    "    measured_counts = torch.poisson(torch.ones(num_lors) * 100)\n",
    "    \n",
    "    # Initialize reconstructor\n",
    "    reconstructor = CUDAMLEMReconstructor(lors, voxel_size=2.0, device='cuda')\n",
    "    \n",
    "    # Create example attenuation mask (uniform attenuation)\n",
    "    grid_shape = reconstructor.grid_size\n",
    "    attenuation_mask = torch.ones(grid_shape[0], grid_shape[1], grid_shape[2]) * 0.1\n",
    "    \n",
    "    # Perform reconstruction\n",
    "    reconstructed_image = reconstructor.reconstruct(\n",
    "        measured_counts, \n",
    "        attenuation_mask,\n",
    "        num_iterations=10\n",
    "    )\n",
    "    \n",
    "    # Convert to 3D for visualization\n",
    "    image_3d = reconstructor.get_image_3d(reconstructed_image)\n",
    "    \n",
    "    print(f\"Reconstructed image shape: {image_3d.shape}\")\n",
    "    print(f\"Image statistics: min={image_3d.min():.3f}, max={image_3d.max():.3f}, \"\n",
    "          f\"mean={image_3d.mean():.3f}\")\n",
    "    \n",
    "    return reconstructed_image, image_3d\n",
    "\n",
    "\n",
    "def MLEM(input_data):\n",
    "    \n",
    "    \n",
    "    # Generate example measured data\n",
    "    measured_counts = torch.poisson(torch.ones(input_data.shape[0]) * 100)\n",
    "    \n",
    "    # Initialize reconstructor\n",
    "    reconstructor = CUDAMLEMReconstructor(input_data, voxel_size=2.0, device='cuda')\n",
    "    \n",
    "    # Create example attenuation mask (uniform attenuation)\n",
    "    grid_shape = reconstructor.grid_size\n",
    "    attenuation_mask = torch.ones(grid_shape[0], grid_shape[1], grid_shape[2]) * 0.1\n",
    "    \n",
    "    # Perform reconstruction\n",
    "    reconstructed_image = reconstructor.reconstruct(\n",
    "        measured_counts, \n",
    "        attenuation_mask,\n",
    "        num_iterations=1\n",
    "    )\n",
    "    \n",
    "    # Convert to 3D for visualization\n",
    "    image_3d = reconstructor.get_image_3d(reconstructed_image)\n",
    "    \n",
    "    print(f\"Reconstructed image shape: {image_3d.shape}\")\n",
    "    print(f\"Image statistics: min={image_3d.min():.3f}, max={image_3d.max():.3f}, \"\n",
    "          f\"mean={image_3d.mean():.3f}\")\n",
    "    \n",
    "    return reconstructed_image, image_3d\n",
    "\n",
    "\n",
    "# LOAD LOR DATA\n",
    "# Initial Loading, Filtering, and Coordinate Range Calculation\n",
    "import numpy as np\n",
    "\n",
    "# Load coordinates\n",
    "coordinates = np.load(fr\"C:\\Users\\h\\Desktop\\PetStuff\\Image_Processing\\ground_truth.npy\")\n",
    "\n",
    "# Confirm shape should be (pairs, coords=6), coords are (x1, y1, z1, x2, y2, z2)\n",
    "print(f\"\\nData Shape (pairs, coords) : {coordinates.shape}\\n\")  \n",
    "\n",
    "# Remove pairs where any coordinate value is exactly 0\n",
    "filtered_coordinates = coordinates[~np.any(coordinates == 0, axis=1)]\n",
    "print(f\"Filtered shape: {filtered_coordinates.shape}\\n\")\n",
    "filtered_coordinates = torch.from_numpy(filtered_coordinates).float()\n",
    "\n",
    "# Extract all x, y, z pairs - Coordinates are in the order (x1, y1, z1, x2, y2, z2)\n",
    "all_xyz = filtered_coordinates.reshape(-1, 3) # Reshape to (pairs, 3) for (x, y, z)\n",
    "x_vals, y_vals, z_vals = all_xyz[:, 0], all_xyz[:, 1], all_xyz[:, 2]\n",
    "print(f\"x range: min={x_vals.min()}, max={x_vals.max()}\")\n",
    "print(f\"y range: min={y_vals.min()}, max={y_vals.max()}\")\n",
    "print(f\"z range: min={z_vals.min()}, max={z_vals.max()}\")\n",
    "\n",
    "image, image3d = MLEM(filtered_coordinates[:100,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0dc5620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69b37a3c6624bcf982a03a4d40a05a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=138, description='X index', max=275), IntSlider(value=137, description='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_cross_sections_horizontal(x_idx=138, y_idx=137, z_idx=72, vmax=0.5, cmap='Magma')>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "from ipywidgets import interact, IntSlider, FloatSlider\n",
    "\n",
    "# Convert tensor to numpy for plotting\n",
    "recon_np = image3d.cpu().numpy()\n",
    "nx, ny, nz = recon_np.shape\n",
    "\n",
    "def plot_cross_sections_horizontal(x_idx=nx//2, y_idx=ny//2, z_idx=nz//2, vmax=0.5, cmap='Magma'):\n",
    "    fig = make_subplots(rows=1, cols=3, subplot_titles=[\n",
    "        f'XY plane @ z={z_idx}',\n",
    "        f'XZ plane @ y={y_idx}',\n",
    "        f'YZ plane @ x={x_idx}'\n",
    "    ])\n",
    "\n",
    "    # XY plane at z=z_idx\n",
    "    fig.add_trace(go.Heatmap(\n",
    "        z=recon_np[:, :, z_idx].T,\n",
    "        colorscale=cmap,\n",
    "        zmax=vmax,\n",
    "        zmin=0,\n",
    "        showscale=True,\n",
    "        name=f'XY @ z={z_idx}'\n",
    "    ), row=1, col=1)\n",
    "\n",
    "    # XZ plane at y=y_idx\n",
    "    fig.add_trace(go.Heatmap(\n",
    "        z=recon_np[:, y_idx, :].T,\n",
    "        colorscale=cmap,\n",
    "        zmax=vmax,\n",
    "        zmin=0,\n",
    "        showscale=True,\n",
    "        name=f'XZ @ y={y_idx}'\n",
    "    ), row=1, col=2)\n",
    "\n",
    "    # YZ plane at x=x_idx\n",
    "    fig.add_trace(go.Heatmap(\n",
    "        z=recon_np[x_idx, :, :].T,\n",
    "        colorscale=cmap,\n",
    "        zmax=vmax,\n",
    "        zmin=0,\n",
    "        showscale=True,\n",
    "        name=f'YZ @ x={x_idx}'\n",
    "    ), row=1, col=3)\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=1200,\n",
    "        height=400,\n",
    "        title_text=\"Orthogonal Cross Sections\"\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "interact(\n",
    "    plot_cross_sections_horizontal,\n",
    "    x_idx=IntSlider(min=0, max=nx-1, step=1, value=nx//2, description='X index'),\n",
    "    y_idx=IntSlider(min=0, max=ny-1, step=1, value=ny//2, description='Y index'),\n",
    "    z_idx=IntSlider(min=0, max=nz-1, step=1, value=nz//2, description='Z index'),\n",
    "    vmax=FloatSlider(min=0, max=1, step=0.01, value=0.05, description='vmax'),\n",
    "    cmap=['Magma','Greys', 'Viridis', 'Cividis', 'Plasma']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0d78358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voxel value range: 2.795579327507096e-25 to 4.270153045654297\n",
      "Total non-zero voxels: 8172\n",
      "Initial thresholds: 2.795579327507096e-25 to 0.5\n",
      "Slider range: 2.795579327507096e-25 to 4.270153045654297\n",
      "Voxel resolution: 1.0mm\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d75f3d5121440981d51982105918bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=2.795579327507096e-25, continuous_update=False, description='Min Thres…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def visualize_voxel_tensor_3d(voxel_tensor, initial_min_threshold=None, initial_max_threshold=None, \n",
    "                               voxel_size_mm=1.0, world_origin=None, min_threshold=None, max_threshold=None):\n",
    "    \"\"\"\n",
    "    Interactive 3D visualization of voxel tensor with dual threshold sliders.\n",
    "\n",
    "    Args:\n",
    "        voxel_tensor: (nx, ny, nz) numpy array with voxel counts\n",
    "        initial_min_threshold: Initial minimum threshold value for the slider (default: min_val)\n",
    "        initial_max_threshold: Initial maximum threshold value for the slider (default: max_val)\n",
    "        voxel_size_mm: Size of each voxel in mm (default: 1.0mm)\n",
    "        world_origin: (x_min, y_min, z_min) world coordinates of voxel (0,0,0) (optional)\n",
    "        min_threshold: Minimum threshold value for slider range (optional)\n",
    "        max_threshold: Maximum threshold value for slider range (optional)\n",
    "    \"\"\"\n",
    "    # Extract non-zero voxel coordinates and values\n",
    "    coords = np.where(voxel_tensor > 0)\n",
    "    x_coords, y_coords, z_coords = coords\n",
    "    values = voxel_tensor[coords]\n",
    "\n",
    "    # Convert voxel indices to world coordinates if world_origin provided\n",
    "    if world_origin is not None:\n",
    "        x_min, y_min, z_min = world_origin\n",
    "        x_coords_world = x_coords * voxel_size_mm + x_min\n",
    "        y_coords_world = y_coords * voxel_size_mm + y_min\n",
    "        z_coords_world = z_coords * voxel_size_mm + z_min\n",
    "        coord_suffix = \" (mm)\"\n",
    "    else:\n",
    "        x_coords_world = x_coords * voxel_size_mm\n",
    "        y_coords_world = y_coords * voxel_size_mm\n",
    "        z_coords_world = z_coords * voxel_size_mm\n",
    "        coord_suffix = f\" (×{voxel_size_mm}mm)\"\n",
    "\n",
    "    # Get value range for sliders\n",
    "    min_val = float(np.min(values))\n",
    "    max_val = float(np.max(values))\n",
    "\n",
    "    # Use user-specified min/max threshold range if provided\n",
    "    slider_min = min_threshold if min_threshold is not None else min_val\n",
    "    slider_max = max_threshold if max_threshold is not None else max_val\n",
    "\n",
    "    # Set initial thresholds with defaults\n",
    "    if initial_min_threshold is None:\n",
    "        initial_min_threshold = slider_min\n",
    "    else:\n",
    "        initial_min_threshold = max(slider_min, min(slider_max, float(initial_min_threshold)))\n",
    "    \n",
    "    if initial_max_threshold is None:\n",
    "        initial_max_threshold = slider_max\n",
    "    else:\n",
    "        initial_max_threshold = max(slider_min, min(slider_max, float(initial_max_threshold)))\n",
    "\n",
    "    # Ensure min <= max\n",
    "    if initial_min_threshold > initial_max_threshold:\n",
    "        initial_min_threshold, initial_max_threshold = initial_max_threshold, initial_min_threshold\n",
    "\n",
    "    print(f\"Voxel value range: {min_val} to {max_val}\")\n",
    "    print(f\"Total non-zero voxels: {len(values)}\")\n",
    "    print(f\"Initial thresholds: {initial_min_threshold} to {initial_max_threshold}\")\n",
    "    print(f\"Slider range: {slider_min} to {slider_max}\")\n",
    "    print(f\"Voxel resolution: {voxel_size_mm}mm\")\n",
    "\n",
    "    def update_plot(min_thresh, max_thresh):\n",
    "        # Ensure min <= max\n",
    "        if min_thresh > max_thresh:\n",
    "            min_thresh, max_thresh = max_thresh, min_thresh\n",
    "\n",
    "        # Filter voxels within threshold range\n",
    "        mask = (values >= min_thresh) & (values <= max_thresh)\n",
    "        if not np.any(mask):\n",
    "            print(f\"No voxels in threshold range [{min_thresh}, {max_thresh}]\")\n",
    "            return\n",
    "\n",
    "        filtered_x = x_coords_world[mask]\n",
    "        filtered_y = y_coords_world[mask]\n",
    "        filtered_z = z_coords_world[mask]\n",
    "        filtered_values = values[mask]\n",
    "\n",
    "        # Create 3D scatter plot\n",
    "        fig = go.Figure(data=go.Scatter3d(\n",
    "            x=filtered_x,\n",
    "            y=filtered_y,\n",
    "            z=filtered_z,\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=1,\n",
    "                color=filtered_values,\n",
    "                colorscale='Viridis',\n",
    "                opacity=0.8,\n",
    "                colorbar=dict(title=\"Voxel Count\"),\n",
    "                line=dict(width=0)\n",
    "            ),\n",
    "            text=[f'Count: {v}' for v in filtered_values],\n",
    "            hovertemplate='<b>Voxel (%{x:.1f}, %{y:.1f}, %{z:.1f})</b><br>%{text}<extra></extra>'\n",
    "        ))\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f'3D Voxel Visualization (Range: [{min_thresh:.6f}, {max_thresh:.6f}], Showing: {len(filtered_values)} voxels)',\n",
    "            scene=dict(\n",
    "                xaxis_title=f'X{coord_suffix}',\n",
    "                yaxis_title=f'Y{coord_suffix}',\n",
    "                zaxis_title=f'Z{coord_suffix}',\n",
    "                camera=dict(\n",
    "                    eye=dict(x=1.5, y=1.5, z=1.5)\n",
    "                ),\n",
    "                aspectmode='cube'\n",
    "            ),\n",
    "            width=800,\n",
    "            height=600\n",
    "        )\n",
    "\n",
    "        fig.show()\n",
    "\n",
    "    # Create interactive sliders with linked constraints\n",
    "    min_threshold_slider = FloatSlider(\n",
    "        value=initial_min_threshold,\n",
    "        min=slider_min,\n",
    "        max=slider_max,\n",
    "        step=0.01,\n",
    "        description='Min Threshold:',\n",
    "        continuous_update=False,\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    max_threshold_slider = FloatSlider(\n",
    "        value=initial_max_threshold,\n",
    "        min=slider_min,\n",
    "        max=slider_max,\n",
    "        step=0.01,\n",
    "        description='Max Threshold:',\n",
    "        continuous_update=False,\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    # Link sliders to maintain min <= max constraint\n",
    "    def on_min_change(change):\n",
    "        if change['new'] > max_threshold_slider.value:\n",
    "            max_threshold_slider.value = change['new']\n",
    "\n",
    "    def on_max_change(change):\n",
    "        if change['new'] < min_threshold_slider.value:\n",
    "            min_threshold_slider.value = change['new']\n",
    "\n",
    "    min_threshold_slider.observe(on_min_change, names='value')\n",
    "    max_threshold_slider.observe(on_max_change, names='value')\n",
    "\n",
    "    interact(update_plot, \n",
    "             min_thresh=min_threshold_slider, \n",
    "             max_thresh=max_threshold_slider)\n",
    "\n",
    "visualize_voxel_tensor_3d(image3d.cpu().numpy(),\n",
    "                         initial_min_threshold=0.0, \n",
    "                         initial_max_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d07ed674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized grid: [459, 458, 455], 95,651,010 voxels\n",
      "Starting optimized GPU MLEM reconstruction...\n",
      "Building system matrix with GPU acceleration...\n",
      "Total intersections: 0\n",
      "System matrix: (100000, 95651010), 0 non-zeros\n",
      "Iteration 1: Log-likelihood = -14987221.00\n",
      "Iteration 2: (likelihood check skipped for speed)\n",
      "Iteration 3: Log-likelihood = -14987221.00, Rel. change = 0.000000\n",
      "Converged after 3 iterations\n",
      "\n",
      "Reconstruction completed in 11.26 seconds\n",
      "Final image shape: torch.Size([459, 458, 455])\n",
      "Image statistics: min=0.000, max=0.000, mean=0.000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from typing import Tuple, Optional\n",
    "import math\n",
    "import cupy as cp\n",
    "from cupyx.scipy.sparse import csr_matrix as cupy_csr_matrix\n",
    "\n",
    "class OptimizedCUDAMLEM:\n",
    "    \"\"\"\n",
    "    Highly optimized CUDA-accelerated MLEM with custom CUDA kernels,\n",
    "    memory-efficient operations, and vectorized computations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lors: torch.Tensor, voxel_size: float = 2.0, device: str = 'cuda'):\n",
    "        \"\"\"Initialize with maximum GPU optimization.\"\"\"\n",
    "        self.device = torch.device(device)\n",
    "        self.lors = lors.to(self.device, dtype=torch.float32, non_blocking=True)\n",
    "        self.voxel_size = voxel_size\n",
    "        self.num_lors = lors.shape[0]\n",
    "        \n",
    "        # Pre-allocate GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Calculate voxel space\n",
    "        self._calculate_voxel_space()\n",
    "        \n",
    "        # Pre-compile CUDA kernels\n",
    "        self._setup_cuda_kernels()\n",
    "        \n",
    "        # System matrix storage\n",
    "        self.system_matrix_csr = None\n",
    "        self.sensitivity_image = None\n",
    "        \n",
    "    def _calculate_voxel_space(self):\n",
    "        \"\"\"Optimized voxel space calculation.\"\"\"\n",
    "        # Vectorized min/max calculation\n",
    "        coords = self.lors.view(-1, 3)\n",
    "        min_coords, _ = torch.min(coords, dim=0)\n",
    "        max_coords, _ = torch.max(coords, dim=0)\n",
    "        \n",
    "        # Add padding\n",
    "        padding = self.voxel_size\n",
    "        min_coords -= padding\n",
    "        max_coords += padding\n",
    "        \n",
    "        # Calculate grid dimensions\n",
    "        self.grid_size = ((max_coords - min_coords) / self.voxel_size).ceil().int()\n",
    "        self.grid_origin = min_coords\n",
    "        self.total_voxels = self.grid_size.prod().item()\n",
    "        \n",
    "        print(f\"Optimized grid: {self.grid_size.tolist()}, {self.total_voxels:,} voxels\")\n",
    "        \n",
    "    def _setup_cuda_kernels(self):\n",
    "        \"\"\"Setup custom CUDA kernels for maximum performance.\"\"\"\n",
    "        # Ray tracing kernel\n",
    "        self.ray_trace_kernel = cp.RawKernel(r'''\n",
    "        extern \"C\" __global__\n",
    "        void siddon_ray_trace(const float* lors, const float* grid_origin, \n",
    "                             const int* grid_size, float voxel_size,\n",
    "                             int* voxel_indices, int* lor_indices, \n",
    "                             float* weights, int* counts, int num_lors) {\n",
    "            \n",
    "            int lor_idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "            if (lor_idx >= num_lors) return;\n",
    "            \n",
    "            // Get LOR endpoints\n",
    "            float x1 = lors[lor_idx * 6 + 0];\n",
    "            float y1 = lors[lor_idx * 6 + 1]; \n",
    "            float z1 = lors[lor_idx * 6 + 2];\n",
    "            float x2 = lors[lor_idx * 6 + 3];\n",
    "            float y2 = lors[lor_idx * 6 + 4];\n",
    "            float z2 = lors[lor_idx * 6 + 5];\n",
    "            \n",
    "            // Convert to voxel coordinates\n",
    "            float p1x = (x1 - grid_origin[0]) / voxel_size;\n",
    "            float p1y = (y1 - grid_origin[1]) / voxel_size;\n",
    "            float p1z = (z1 - grid_origin[2]) / voxel_size;\n",
    "            float p2x = (x2 - grid_origin[0]) / voxel_size;\n",
    "            float p2y = (y2 - grid_origin[1]) / voxel_size;\n",
    "            float p2z = (z2 - grid_origin[2]) / voxel_size;\n",
    "            \n",
    "            // Ray direction\n",
    "            float dx = p2x - p1x;\n",
    "            float dy = p2y - p1y;\n",
    "            float dz = p2z - p1z;\n",
    "            float ray_length = sqrtf(dx*dx + dy*dy + dz*dz);\n",
    "            \n",
    "            if (ray_length < 1e-6) return;\n",
    "            \n",
    "            dx /= ray_length;\n",
    "            dy /= ray_length; \n",
    "            dz /= ray_length;\n",
    "            \n",
    "            // Siddon algorithm - simplified for speed\n",
    "            float stepX = (dx > 0) ? 1.0f : -1.0f;\n",
    "            float stepY = (dy > 0) ? 1.0f : -1.0f;\n",
    "            float stepZ = (dz > 0) ? 1.0f : -1.0f;\n",
    "            \n",
    "            float tDeltaX = fabsf(1.0f / dx);\n",
    "            float tDeltaY = fabsf(1.0f / dy);\n",
    "            float tDeltaZ = fabsf(1.0f / dz);\n",
    "            \n",
    "            int voxelX = (int)floorf(p1x);\n",
    "            int voxelY = (int)floorf(p1y);\n",
    "            int voxelZ = (int)floorf(p1z);\n",
    "            \n",
    "            float tMaxX = (dx > 0) ? (voxelX + 1 - p1x) * tDeltaX : (p1x - voxelX) * tDeltaX;\n",
    "            float tMaxY = (dy > 0) ? (voxelY + 1 - p1y) * tDeltaY : (p1y - voxelY) * tDeltaY;\n",
    "            float tMaxZ = (dz > 0) ? (voxelZ + 1 - p1z) * tDeltaZ : (p1z - voxelZ) * tDeltaZ;\n",
    "            \n",
    "            float t = 0.0f;\n",
    "            int count = 0;\n",
    "            int max_steps = grid_size[0] + grid_size[1] + grid_size[2];\n",
    "            \n",
    "            while (t < ray_length && count < max_steps) {\n",
    "                // Check bounds\n",
    "                if (voxelX >= 0 && voxelX < grid_size[0] &&\n",
    "                    voxelY >= 0 && voxelY < grid_size[1] &&\n",
    "                    voxelZ >= 0 && voxelZ < grid_size[2]) {\n",
    "                    \n",
    "                    float t_next = fminf(fminf(tMaxX, tMaxY), tMaxZ);\n",
    "                    t_next = fminf(t_next, ray_length);\n",
    "                    float length = t_next - t;\n",
    "                    \n",
    "                    if (length > 1e-6) {\n",
    "                        int voxel_idx = voxelX * grid_size[1] * grid_size[2] + \n",
    "                                       voxelY * grid_size[2] + voxelZ;\n",
    "                        \n",
    "                        int base_idx = lor_idx * max_steps + count;\n",
    "                        voxel_indices[base_idx] = voxel_idx;\n",
    "                        lor_indices[base_idx] = lor_idx;\n",
    "                        weights[base_idx] = length * voxel_size;\n",
    "                        count++;\n",
    "                    }\n",
    "                    \n",
    "                    t = t_next;\n",
    "                }\n",
    "                \n",
    "                // Move to next voxel\n",
    "                if (tMaxX < tMaxY && tMaxX < tMaxZ) {\n",
    "                    voxelX += (int)stepX;\n",
    "                    tMaxX += tDeltaX;\n",
    "                } else if (tMaxY < tMaxZ) {\n",
    "                    voxelY += (int)stepY;\n",
    "                    tMaxY += tDeltaY;\n",
    "                } else {\n",
    "                    voxelZ += (int)stepZ;\n",
    "                    tMaxZ += tDeltaZ;\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            counts[lor_idx] = count;\n",
    "        }\n",
    "        ''', 'siddon_ray_trace')\n",
    "        \n",
    "        # Forward projection kernel\n",
    "        self.forward_proj_kernel = cp.RawKernel(r'''\n",
    "        extern \"C\" __global__\n",
    "        void forward_project(const float* image, const int* row_ptr, \n",
    "                           const int* col_idx, const float* data,\n",
    "                           float* sino, int num_lors) {\n",
    "            int lor_idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "            if (lor_idx >= num_lors) return;\n",
    "            \n",
    "            float sum = 0.0f;\n",
    "            for (int i = row_ptr[lor_idx]; i < row_ptr[lor_idx + 1]; i++) {\n",
    "                sum += data[i] * image[col_idx[i]];\n",
    "            }\n",
    "            sino[lor_idx] = sum;\n",
    "        }\n",
    "        ''', 'forward_project')\n",
    "        \n",
    "        # Back projection kernel  \n",
    "        self.back_proj_kernel = cp.RawKernel(r'''\n",
    "        extern \"C\" __global__\n",
    "        void back_project(const float* sino, const int* row_ptr,\n",
    "                         const int* col_idx, const float* data, \n",
    "                         float* image, int num_lors) {\n",
    "            int lor_idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "            if (lor_idx >= num_lors) return;\n",
    "            \n",
    "            for (int i = row_ptr[lor_idx]; i < row_ptr[lor_idx + 1]; i++) {\n",
    "                atomicAdd(&image[col_idx[i]], data[i] * sino[lor_idx]);\n",
    "            }\n",
    "        }\n",
    "        ''', 'back_project')\n",
    "    \n",
    "    def build_system_matrix_gpu(self, max_intersections_per_lor: int = 1000):\n",
    "        \"\"\"Ultra-fast GPU system matrix building with custom CUDA kernels.\"\"\"\n",
    "        print(\"Building system matrix with GPU acceleration...\")\n",
    "        \n",
    "        # Allocate GPU memory for intersections\n",
    "        max_total = self.num_lors * max_intersections_per_lor\n",
    "        voxel_indices = cp.zeros(max_total, dtype=cp.int32)\n",
    "        lor_indices = cp.zeros(max_total, dtype=cp.int32)\n",
    "        weights = cp.zeros(max_total, dtype=cp.float32)\n",
    "        counts = cp.zeros(self.num_lors, dtype=cp.int32)\n",
    "        \n",
    "        # Convert tensors to CuPy arrays\n",
    "        lors_cp = cp.asarray(self.lors)\n",
    "        grid_origin_cp = cp.asarray(self.grid_origin)\n",
    "        grid_size_cp = cp.asarray(self.grid_size)\n",
    "        \n",
    "        # Launch ray tracing kernel\n",
    "        threads_per_block = 256\n",
    "        blocks = (self.num_lors + threads_per_block - 1) // threads_per_block\n",
    "        \n",
    "        self.ray_trace_kernel(\n",
    "            (blocks,), (threads_per_block,),\n",
    "            (lors_cp, grid_origin_cp, grid_size_cp, self.voxel_size,\n",
    "             voxel_indices, lor_indices, weights, counts, self.num_lors)\n",
    "        )\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        \n",
    "        # Compact results (remove zeros)\n",
    "        total_intersections = int(cp.sum(counts))\n",
    "        print(f\"Total intersections: {total_intersections:,}\")\n",
    "        \n",
    "        # Create compact arrays\n",
    "        valid_voxels = []\n",
    "        valid_lors = []\n",
    "        valid_weights = []\n",
    "        \n",
    "        for lor_idx in range(self.num_lors):\n",
    "            count = int(counts[lor_idx])\n",
    "            if count > 0:\n",
    "                start_idx = lor_idx * max_intersections_per_lor\n",
    "                end_idx = start_idx + count\n",
    "                valid_voxels.extend(voxel_indices[start_idx:end_idx].tolist())\n",
    "                valid_lors.extend([lor_idx] * count)\n",
    "                valid_weights.extend(weights[start_idx:end_idx].tolist())\n",
    "        \n",
    "        # Create CSR matrix for ultra-fast SpMV\n",
    "        row_indices = cp.array(valid_lors, dtype=cp.int32)\n",
    "        col_indices = cp.array(valid_voxels, dtype=cp.int32)\n",
    "        data = cp.array(valid_weights, dtype=cp.float32)\n",
    "        \n",
    "        self.system_matrix_csr = cupy_csr_matrix(\n",
    "            (data, (row_indices, col_indices)), \n",
    "            shape=(self.num_lors, self.total_voxels)\n",
    "        )\n",
    "        \n",
    "        # Calculate sensitivity image (column sums) - GPU accelerated\n",
    "        self.sensitivity_image = cp.array(self.system_matrix_csr.sum(axis=0)).flatten()\n",
    "        self.sensitivity_image[self.sensitivity_image == 0] = 1.0\n",
    "        \n",
    "        print(f\"System matrix: {self.system_matrix_csr.shape}, \"\n",
    "              f\"{self.system_matrix_csr.nnz:,} non-zeros\")\n",
    "    \n",
    "    def forward_project_gpu(self, image_cp: cp.ndarray) -> cp.ndarray:\n",
    "        \"\"\"Ultra-fast GPU forward projection using custom CUDA kernel.\"\"\"\n",
    "        sino = cp.zeros(self.num_lors, dtype=cp.float32)\n",
    "        \n",
    "        threads_per_block = 256\n",
    "        blocks = (self.num_lors + threads_per_block - 1) // threads_per_block\n",
    "        \n",
    "        self.forward_proj_kernel(\n",
    "            (blocks,), (threads_per_block,),\n",
    "            (image_cp, self.system_matrix_csr.indptr, \n",
    "             self.system_matrix_csr.indices, self.system_matrix_csr.data,\n",
    "             sino, self.num_lors)\n",
    "        )\n",
    "        \n",
    "        return sino\n",
    "    \n",
    "    def back_project_gpu(self, sino_cp: cp.ndarray) -> cp.ndarray:\n",
    "        \"\"\"Ultra-fast GPU back projection using custom CUDA kernel.\"\"\"\n",
    "        image = cp.zeros(self.total_voxels, dtype=cp.float32)\n",
    "        \n",
    "        threads_per_block = 256\n",
    "        blocks = (self.num_lors + threads_per_block - 1) // threads_per_block\n",
    "        \n",
    "        self.back_proj_kernel(\n",
    "            (blocks,), (threads_per_block,),\n",
    "            (sino_cp, self.system_matrix_csr.indptr,\n",
    "             self.system_matrix_csr.indices, self.system_matrix_csr.data,\n",
    "             image, self.num_lors)\n",
    "        )\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def estimate_scatter_gpu(self, image_cp: cp.ndarray, scatter_fraction: float = 0.15) -> cp.ndarray:\n",
    "        \"\"\"GPU-accelerated scatter estimation with 3D convolution.\"\"\"\n",
    "        # Reshape to 3D\n",
    "        img_3d = image_cp.reshape(self.grid_size[0].item(), \n",
    "                                 self.grid_size[1].item(), \n",
    "                                 self.grid_size[2].item())\n",
    "        \n",
    "        # Fast 3D Gaussian convolution using separable filters\n",
    "        sigma = 2.0\n",
    "        kernel_size = 7\n",
    "        \n",
    "        # Create 1D Gaussian kernel\n",
    "        x = cp.arange(kernel_size) - kernel_size // 2\n",
    "        kernel_1d = cp.exp(-x**2 / (2 * sigma**2))\n",
    "        kernel_1d /= kernel_1d.sum()\n",
    "        \n",
    "        # Apply separable 3D convolution (much faster than full 3D)\n",
    "        from cupyx.scipy.ndimage import convolve1d\n",
    "        \n",
    "        # Convolve along each axis\n",
    "        scattered = convolve1d(img_3d, kernel_1d, axis=0, mode='constant')\n",
    "        scattered = convolve1d(scattered, kernel_1d, axis=1, mode='constant')\n",
    "        scattered = convolve1d(scattered, kernel_1d, axis=2, mode='constant')\n",
    "        \n",
    "        # Forward project and scale\n",
    "        scattered_flat = scattered.flatten()\n",
    "        scatter_sino = self.forward_project_gpu(scattered_flat)\n",
    "        \n",
    "        return scatter_sino * scatter_fraction\n",
    "    \n",
    "    def reconstruct_gpu(self, measured_data: torch.Tensor, \n",
    "                       attenuation_mask: torch.Tensor,\n",
    "                       num_iterations: int = 20, \n",
    "                       convergence_threshold: float = 1e-4) -> torch.Tensor:\n",
    "        \"\"\"Ultra-fast GPU MLEM reconstruction.\"\"\"\n",
    "        print(\"Starting optimized GPU MLEM reconstruction...\")\n",
    "        \n",
    "        # Build system matrix if needed\n",
    "        if self.system_matrix_csr is None:\n",
    "            self.build_system_matrix_gpu()\n",
    "        \n",
    "        # Convert to CuPy for maximum speed\n",
    "        measured_cp = cp.asarray(measured_data.to(self.device))\n",
    "        atten_cp = cp.asarray(attenuation_mask.to(self.device).flatten())\n",
    "        \n",
    "        # Initialize image\n",
    "        image_cp = cp.ones(self.total_voxels, dtype=cp.float32)\n",
    "        \n",
    "        # Apply attenuation correction to measured data\n",
    "        atten_factors = self.forward_project_gpu(atten_cp)\n",
    "        atten_correction = cp.exp(-atten_factors)\n",
    "        measured_corrected = measured_cp * atten_correction\n",
    "        \n",
    "        # Pre-allocate arrays for speed\n",
    "        estimated_sino = cp.zeros(self.num_lors, dtype=cp.float32)\n",
    "        correction = cp.zeros(self.total_voxels, dtype=cp.float32)\n",
    "        \n",
    "        prev_likelihood = float('inf')\n",
    "        \n",
    "        # MLEM iterations\n",
    "        for iteration in range(num_iterations):\n",
    "            # Forward project - GPU accelerated\n",
    "            estimated_sino = self.forward_project_gpu(image_cp)\n",
    "            \n",
    "            # Add scatter and randoms - GPU operations\n",
    "            if iteration % 2 == 0:  # Update scatter every other iteration for speed\n",
    "                scatter = self.estimate_scatter_gpu(image_cp)\n",
    "                randoms = cp.full(self.num_lors, 0.05, dtype=cp.float32)\n",
    "                self._cached_corrections = scatter + randoms\n",
    "            \n",
    "            total_estimated = estimated_sino + self._cached_corrections\n",
    "            total_estimated = cp.maximum(total_estimated, 1e-10)\n",
    "            \n",
    "            # Calculate ratio and back project - GPU accelerated\n",
    "            ratio = measured_corrected / total_estimated\n",
    "            correction = self.back_project_gpu(ratio)\n",
    "            \n",
    "            # MLEM update - GPU operations\n",
    "            image_cp *= correction / self.sensitivity_image\n",
    "            image_cp = cp.maximum(image_cp, 0)  # Non-negativity\n",
    "            \n",
    "            # Fast likelihood calculation (subset for speed)\n",
    "            if iteration % 2 == 0:\n",
    "                likelihood = float(cp.sum(measured_corrected * cp.log(total_estimated) - total_estimated))\n",
    "                \n",
    "                if iteration > 0:\n",
    "                    rel_change = abs(likelihood - prev_likelihood) / abs(prev_likelihood)\n",
    "                    print(f\"Iteration {iteration + 1}: Log-likelihood = {likelihood:.2f}, \"\n",
    "                          f\"Rel. change = {rel_change:.6f}\")\n",
    "                    \n",
    "                    if rel_change < convergence_threshold:\n",
    "                        print(f\"Converged after {iteration + 1} iterations\")\n",
    "                        break\n",
    "                else:\n",
    "                    print(f\"Iteration {iteration + 1}: Log-likelihood = {likelihood:.2f}\")\n",
    "                \n",
    "                prev_likelihood = likelihood\n",
    "            else:\n",
    "                print(f\"Iteration {iteration + 1}: (likelihood check skipped for speed)\")\n",
    "        \n",
    "        # Convert back to PyTorch\n",
    "        result = torch.as_tensor(image_cp, device=self.device)\n",
    "        return result\n",
    "    \n",
    "    def get_image_3d(self, image_1d: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Convert 1D image to 3D array.\"\"\"\n",
    "        return image_1d.view(self.grid_size[0], self.grid_size[1], self.grid_size[2])\n",
    "\n",
    "# Optimized example usage\n",
    "def optimized_example():\n",
    "    \"\"\"Example with GPU optimization features.\"\"\"\n",
    "    \n",
    "    # Enable GPU optimizations\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Generate test data\n",
    "    num_lors = 100000  # Larger dataset to show performance gains\n",
    "    lors = torch.randn(num_lors, 6, device='cuda') * 100\n",
    "    measured_counts = torch.poisson(torch.ones(num_lors, device='cuda') * 50)\n",
    "    \n",
    "    # Initialize optimized reconstructor\n",
    "    reconstructor = OptimizedCUDAMLEM(lors, voxel_size=2.0, device='cuda')\n",
    "    \n",
    "    # Create attenuation mask\n",
    "    grid_shape = reconstructor.grid_size\n",
    "    attenuation_mask = torch.ones(grid_shape[0], grid_shape[1], grid_shape[2], \n",
    "                                device='cuda') * 0.1\n",
    "    \n",
    "    # Time the reconstruction\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Perform optimized reconstruction\n",
    "    reconstructed_image = reconstructor.reconstruct_gpu(\n",
    "        measured_counts, \n",
    "        attenuation_mask,\n",
    "        num_iterations=10\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nReconstruction completed in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Convert to 3D\n",
    "    image_3d = reconstructor.get_image_3d(reconstructed_image)\n",
    "    \n",
    "    print(f\"Final image shape: {image_3d.shape}\")\n",
    "    print(f\"Image statistics: min={image_3d.min():.3f}, max={image_3d.max():.3f}, \"\n",
    "          f\"mean={image_3d.mean():.3f}\")\n",
    "    \n",
    "    return reconstructed_image, image_3d\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    optimized_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebbc5f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized grid: [281, 281, 150], 11,844,150 voxels\n",
      "Starting optimized GPU MLEM reconstruction...\n",
      "Building system matrix with GPU acceleration...\n",
      "Total intersections: 0\n",
      "System matrix: (6591, 11844150), 0 non-zeros\n",
      "Iteration 1: Log-likelihood = -987962.62\n",
      "Iteration 2: (likelihood check skipped for speed)\n",
      "Iteration 3: Log-likelihood = -987962.62, Rel. change = 0.000000\n",
      "Converged after 3 iterations\n",
      "\n",
      "Reconstruction completed in 0.26 seconds\n",
      "Final image shape: torch.Size([281, 281, 150])\n",
      "Image statistics: min=0.000, max=0.000, mean=0.000\n"
     ]
    }
   ],
   "source": [
    "# Enable GPU optimizations\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Generate test data\n",
    "measured_counts = torch.poisson(torch.ones(filtered_coordinates.shape[0], device='cuda') * 50)\n",
    "\n",
    "# Initialize optimized reconstructor\n",
    "reconstructor = OptimizedCUDAMLEM(filtered_coordinates, voxel_size=2.0, device='cuda')\n",
    "\n",
    "# Create attenuation mask\n",
    "grid_shape = reconstructor.grid_size\n",
    "attenuation_mask = torch.ones(grid_shape[0], grid_shape[1], grid_shape[2], \n",
    "                            device='cuda') * 0.1\n",
    "\n",
    "# Time the reconstruction\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform optimized reconstruction\n",
    "reconstructed_image = reconstructor.reconstruct_gpu(\n",
    "    measured_counts, \n",
    "    attenuation_mask,\n",
    "    num_iterations=10\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nReconstruction completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Convert to 3D\n",
    "image_3d = reconstructor.get_image_3d(reconstructed_image)\n",
    "\n",
    "print(f\"Final image shape: {image_3d.shape}\")\n",
    "print(f\"Image statistics: min={image_3d.min():.3f}, max={image_3d.max():.3f}, \"\n",
    "        f\"mean={image_3d.mean():.3f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b74ebf09",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation minimum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m visualize_voxel_tensor_3d(image_3d\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[0;32m      2\u001b[0m                          initial_min_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, \n\u001b[0;32m      3\u001b[0m                          initial_max_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 34\u001b[0m, in \u001b[0;36mvisualize_voxel_tensor_3d\u001b[1;34m(voxel_tensor, initial_min_threshold, initial_max_threshold, voxel_size_mm, world_origin, min_threshold, max_threshold)\u001b[0m\n\u001b[0;32m     31\u001b[0m     coord_suffix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (×\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvoxel_size_mm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mmm)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Get value range for sliders\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m min_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(np\u001b[38;5;241m.\u001b[39mmin(values))\n\u001b[0;32m     35\u001b[0m max_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(np\u001b[38;5;241m.\u001b[39mmax(values))\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Use user-specified min/max threshold range if provided\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\h\\anaconda3\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3343\u001b[0m, in \u001b[0;36mmin\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   3225\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_min_dispatcher)\n\u001b[0;32m   3226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmin\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[0;32m   3227\u001b[0m         where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[0;32m   3228\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3229\u001b[0m \u001b[38;5;124;03m    Return the minimum of an array or minimum along an axis.\u001b[39;00m\n\u001b[0;32m   3230\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3341\u001b[0m \u001b[38;5;124;03m    6\u001b[39;00m\n\u001b[0;32m   3342\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapreduction(a, np\u001b[38;5;241m.\u001b[39mminimum, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, axis, \u001b[38;5;28;01mNone\u001b[39;00m, out,\n\u001b[0;32m   3344\u001b[0m                           keepdims\u001b[38;5;241m=\u001b[39mkeepdims, initial\u001b[38;5;241m=\u001b[39minitial, where\u001b[38;5;241m=\u001b[39mwhere)\n",
      "File \u001b[1;32mc:\\Users\\h\\anaconda3\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation minimum which has no identity"
     ]
    }
   ],
   "source": [
    "visualize_voxel_tensor_3d(image_3d.cpu().numpy(),\n",
    "                         initial_min_threshold=0.0, \n",
    "                         initial_max_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "726efcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized grid: [490, 477, 479], 111,956,670 voxels\n",
      "Starting optimized GPU MLEM reconstruction...\n",
      "Building system matrix with GPU acceleration...\n",
      "LOR data check:\n",
      "  LOR range: X=[-392.9, 528.4]\n",
      "             Y=[-449.2, 500.0]\n",
      "             Z=[-488.9, 463.3]\n",
      "  Grid origin: [-448.3731384277344, -451.216064453125, -490.93780517578125]\n",
      "  Grid size: [490, 477, 479]\n",
      "  Voxel size: 2.0\n",
      "Total intersections: 100,000\n",
      "Validating 100000 intersection weights...\n",
      "After cleaning: 0 valid intersections\n",
      "ERROR: No valid intersections after cleaning!\n",
      "System matrix created: shape=(100000, 111956670)\n",
      "Data range: min=1.000000, max=1.000000\n",
      "NaN in data: 0\n",
      "Calculating sensitivity image...\n",
      "Sensitivity image stats: min=0.000000, max=1.000000, mean=0.000000, zeros=111956669\n",
      "System matrix: (100000, 111956670), 1 non-zeros\n",
      "System matrix data check:\n",
      "  Data stats: min=1.000000, max=1.000000\n",
      "  NaN count: 0\n",
      "  Inf count: 0\n",
      "  Zero count: 0\n",
      "System matrix nnz: 1\n",
      "Sensitivity image stats: min=0.000000, max=1.000000, mean=0.000000\n",
      "WARNING: 111956669 voxels have near-zero sensitivity\n",
      "Applied sensitivity threshold: 1.00e-10\n",
      "Updated sensitivity stats: min=0.000000, max=1.000000\n",
      "Image initialization scale factor: 4.95e+06\n",
      "Initial image stats: min=4.948e+06, max=4.948e+06, mean=4.948e+06\n",
      "Measured data stats: sum=5002939.0, min=23.000, max=86.000\n",
      "Applying attenuation correction...\n",
      "Attenuation factors: min=0.000, max=0.100, mean=0.000\n",
      "Attenuation correction stats: min=0.904837, max=1.000000\n",
      "Corrected data sum: 5002934.5\n",
      "Iteration 1: Log-likelihood = -462794080.00\n",
      "Iteration 2: (likelihood check skipped)\n",
      "Iteration 3: Log-likelihood = -457896160.00, Rel. change = 0.010583\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 735\u001b[0m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reconstructed_image, image_3d\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 735\u001b[0m     optimized_example()\n",
      "Cell \u001b[1;32mIn[28], line 716\u001b[0m, in \u001b[0;36moptimized_example\u001b[1;34m()\u001b[0m\n\u001b[0;32m    713\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    715\u001b[0m \u001b[38;5;66;03m# Perform optimized reconstruction\u001b[39;00m\n\u001b[1;32m--> 716\u001b[0m reconstructed_image \u001b[38;5;241m=\u001b[39m reconstructor\u001b[38;5;241m.\u001b[39mreconstruct_gpu(\n\u001b[0;32m    717\u001b[0m     measured_counts, \n\u001b[0;32m    718\u001b[0m     attenuation_mask,\n\u001b[0;32m    719\u001b[0m     num_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[0;32m    720\u001b[0m )\n\u001b[0;32m    722\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    723\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mReconstruction completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[28], line 661\u001b[0m, in \u001b[0;36mOptimizedCUDAMLEM.reconstruct_gpu\u001b[1;34m(self, measured_data, attenuation_mask, num_iterations, convergence_threshold)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Log-likelihood = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlikelihood\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    658\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRel. change = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrel_change\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    660\u001b[0m \u001b[38;5;66;03m# Check image stats\u001b[39;00m\n\u001b[1;32m--> 661\u001b[0m img_stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage: min=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mfloat\u001b[39m(image_cp\u001b[38;5;241m.\u001b[39mmin())\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, max=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mfloat\u001b[39m(image_cp\u001b[38;5;241m.\u001b[39mmax())\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, mean=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mfloat\u001b[39m(image_cp\u001b[38;5;241m.\u001b[39mmean())\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_stats\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rel_change \u001b[38;5;241m<\u001b[39m convergence_threshold:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from typing import Tuple, Optional\n",
    "import math\n",
    "import cupy as cp\n",
    "from cupyx.scipy.sparse import csr_matrix as cupy_csr_matrix\n",
    "\n",
    "class OptimizedCUDAMLEM:\n",
    "    \"\"\"\n",
    "    Highly optimized CUDA-accelerated MLEM with custom CUDA kernels,\n",
    "    memory-efficient operations, and vectorized computations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lors: torch.Tensor, voxel_size: float = 2.0, device: str = 'cuda'):\n",
    "        \"\"\"Initialize with maximum GPU optimization.\"\"\"\n",
    "        self.device = torch.device(device)\n",
    "        self.lors = lors.to(self.device, dtype=torch.float32, non_blocking=True)\n",
    "        self.voxel_size = voxel_size\n",
    "        self.num_lors = lors.shape[0]\n",
    "        \n",
    "        # Pre-allocate GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Calculate voxel space\n",
    "        self._calculate_voxel_space()\n",
    "        \n",
    "        # Pre-compile CUDA kernels\n",
    "        self._setup_cuda_kernels()\n",
    "        \n",
    "        # System matrix storage\n",
    "        self.system_matrix_csr = None\n",
    "        self.sensitivity_image = None\n",
    "        \n",
    "    def _calculate_voxel_space(self):\n",
    "        \"\"\"Optimized voxel space calculation.\"\"\"\n",
    "        # Vectorized min/max calculation\n",
    "        coords = self.lors.view(-1, 3)\n",
    "        min_coords, _ = torch.min(coords, dim=0)\n",
    "        max_coords, _ = torch.max(coords, dim=0)\n",
    "        \n",
    "        # Add padding\n",
    "        padding = self.voxel_size\n",
    "        min_coords -= padding\n",
    "        max_coords += padding\n",
    "        \n",
    "        # Calculate grid dimensions\n",
    "        self.grid_size = ((max_coords - min_coords) / self.voxel_size).ceil().int()\n",
    "        self.grid_origin = min_coords\n",
    "        self.total_voxels = self.grid_size.prod().item()\n",
    "        \n",
    "        print(f\"Optimized grid: {self.grid_size.tolist()}, {self.total_voxels:,} voxels\")\n",
    "        \n",
    "    def _setup_cuda_kernels(self):\n",
    "        \"\"\"Setup custom CUDA kernels with proper debugging.\"\"\"\n",
    "        # Robust ray tracing kernel with fixed weight calculation\n",
    "        self.ray_trace_kernel = cp.RawKernel(r'''\n",
    "        extern \"C\" __global__\n",
    "        void siddon_ray_trace(const float* lors, const float* grid_origin, \n",
    "                             const int* grid_size, float voxel_size,\n",
    "                             int* voxel_indices, int* lor_indices, \n",
    "                             float* weights, int* counts, int num_lors,\n",
    "                             int max_intersections_per_lor) {\n",
    "            \n",
    "            int lor_idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "            if (lor_idx >= num_lors) return;\n",
    "            \n",
    "            // Get LOR endpoints\n",
    "            float x1 = lors[lor_idx * 6 + 0];\n",
    "            float y1 = lors[lor_idx * 6 + 1]; \n",
    "            float z1 = lors[lor_idx * 6 + 2];\n",
    "            float x2 = lors[lor_idx * 6 + 3];\n",
    "            float y2 = lors[lor_idx * 6 + 4];\n",
    "            float z2 = lors[lor_idx * 6 + 5];\n",
    "            \n",
    "            // Convert to voxel coordinates\n",
    "            float p1x = (x1 - grid_origin[0]) / voxel_size;\n",
    "            float p1y = (y1 - grid_origin[1]) / voxel_size;\n",
    "            float p1z = (z1 - grid_origin[2]) / voxel_size;\n",
    "            float p2x = (x2 - grid_origin[0]) / voxel_size;\n",
    "            float p2y = (y2 - grid_origin[1]) / voxel_size;\n",
    "            float p2z = (z2 - grid_origin[2]) / voxel_size;\n",
    "            \n",
    "            // Ray direction and length in voxel space\n",
    "            float dx = p2x - p1x;\n",
    "            float dy = p2y - p1y;\n",
    "            float dz = p2z - p1z;\n",
    "            float ray_length_voxels = sqrtf(dx*dx + dy*dy + dz*dz);\n",
    "            \n",
    "            if (ray_length_voxels < 1e-6f) {\n",
    "                counts[lor_idx] = 0;\n",
    "                return;\n",
    "            }\n",
    "            \n",
    "            // Normalize direction\n",
    "            dx /= ray_length_voxels;\n",
    "            dy /= ray_length_voxels; \n",
    "            dz /= ray_length_voxels;\n",
    "            \n",
    "            // DDA-style traversal with fixed step size\n",
    "            int num_steps = (int)(ray_length_voxels * 1.5f) + 50;  // Ensure good sampling\n",
    "            if (num_steps > max_intersections_per_lor) num_steps = max_intersections_per_lor;\n",
    "            if (num_steps < 10) num_steps = 10;  // Minimum sampling\n",
    "            \n",
    "            int count = 0;\n",
    "            int prev_voxel_idx = -1;  // Track previous voxel to avoid duplicates\n",
    "            \n",
    "            // Sample along the ray\n",
    "            for (int step = 0; step < num_steps && count < max_intersections_per_lor - 1; step++) {\n",
    "                float t = (float)step / (float)(num_steps - 1) * ray_length_voxels;\n",
    "                \n",
    "                // Current position along ray in voxel coordinates\n",
    "                float px = p1x + t * dx;\n",
    "                float py = p1y + t * dy;\n",
    "                float pz = p1z + t * dz;\n",
    "                \n",
    "                // Current voxel indices\n",
    "                int vx = (int)floorf(px);\n",
    "                int vy = (int)floorf(py);\n",
    "                int vz = (int)floorf(pz);\n",
    "                \n",
    "                // Check bounds\n",
    "                if (vx >= 0 && vx < grid_size[0] &&\n",
    "                    vy >= 0 && vy < grid_size[1] &&\n",
    "                    vz >= 0 && vz < grid_size[2]) {\n",
    "                    \n",
    "                    // Calculate flat voxel index\n",
    "                    int voxel_idx = vx * grid_size[1] * grid_size[2] + \n",
    "                                   vy * grid_size[2] + vz;\n",
    "                    \n",
    "                    // Only add if this is a new voxel\n",
    "                    if (voxel_idx != prev_voxel_idx) {\n",
    "                        int base_idx = lor_idx * max_intersections_per_lor + count;\n",
    "                        \n",
    "                        voxel_indices[base_idx] = voxel_idx;\n",
    "                        lor_indices[base_idx] = lor_idx;\n",
    "                        \n",
    "                        // Fixed weight calculation: length per step in mm\n",
    "                        float weight = ray_length_voxels * voxel_size / (float)num_steps;\n",
    "                        \n",
    "                        // Ensure weight is finite and positive\n",
    "                        if (weight > 0.0f && isfinite(weight)) {\n",
    "                            weights[base_idx] = weight;\n",
    "                        } else {\n",
    "                            weights[base_idx] = voxel_size * 0.1f;  // Fallback weight\n",
    "                        }\n",
    "                        \n",
    "                        prev_voxel_idx = voxel_idx;\n",
    "                        count++;\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            counts[lor_idx] = count;\n",
    "        }\n",
    "        ''', 'siddon_ray_trace')\n",
    "        \n",
    "        # Forward projection kernel\n",
    "        self.forward_proj_kernel = cp.RawKernel(r'''\n",
    "        extern \"C\" __global__\n",
    "        void forward_project(const float* image, const int* row_ptr, \n",
    "                           const int* col_idx, const float* data,\n",
    "                           float* sino, int num_lors) {\n",
    "            int lor_idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "            if (lor_idx >= num_lors) return;\n",
    "            \n",
    "            float sum = 0.0f;\n",
    "            for (int i = row_ptr[lor_idx]; i < row_ptr[lor_idx + 1]; i++) {\n",
    "                sum += data[i] * image[col_idx[i]];\n",
    "            }\n",
    "            sino[lor_idx] = sum;\n",
    "        }\n",
    "        ''', 'forward_project')\n",
    "        \n",
    "        # Back projection kernel  \n",
    "        self.back_proj_kernel = cp.RawKernel(r'''\n",
    "        extern \"C\" __global__\n",
    "        void back_project(const float* sino, const int* row_ptr,\n",
    "                         const int* col_idx, const float* data, \n",
    "                         float* image, int num_lors) {\n",
    "            int lor_idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "            if (lor_idx >= num_lors) return;\n",
    "            \n",
    "            for (int i = row_ptr[lor_idx]; i < row_ptr[lor_idx + 1]; i++) {\n",
    "                atomicAdd(&image[col_idx[i]], data[i] * sino[lor_idx]);\n",
    "            }\n",
    "        }\n",
    "        ''', 'back_project')\n",
    "    \n",
    "    def build_system_matrix_gpu(self, max_intersections_per_lor: int = 1000):\n",
    "        \"\"\"Ultra-fast GPU system matrix building with custom CUDA kernels.\"\"\"\n",
    "        print(\"Building system matrix with GPU acceleration...\")\n",
    "        \n",
    "        # Allocate GPU memory for intersections\n",
    "        max_total = self.num_lors * max_intersections_per_lor\n",
    "        voxel_indices = cp.zeros(max_total, dtype=cp.int32)\n",
    "        lor_indices = cp.zeros(max_total, dtype=cp.int32)\n",
    "        weights = cp.zeros(max_total, dtype=cp.float32)\n",
    "        counts = cp.zeros(self.num_lors, dtype=cp.int32)\n",
    "        \n",
    "        # Convert tensors to CuPy arrays\n",
    "        lors_cp = cp.asarray(self.lors)\n",
    "        grid_origin_cp = cp.asarray(self.grid_origin)\n",
    "        grid_size_cp = cp.asarray(self.grid_size)\n",
    "        \n",
    "        # Debug: Check LOR data first\n",
    "        print(f\"LOR data check:\")\n",
    "        print(f\"  LOR range: X=[{float(lors_cp[:, 0].min()):.1f}, {float(lors_cp[:, 0].max()):.1f}]\")\n",
    "        print(f\"             Y=[{float(lors_cp[:, 1].min()):.1f}, {float(lors_cp[:, 1].max()):.1f}]\") \n",
    "        print(f\"             Z=[{float(lors_cp[:, 2].min()):.1f}, {float(lors_cp[:, 2].max()):.1f}]\")\n",
    "        print(f\"  Grid origin: {[float(x) for x in grid_origin_cp]}\")\n",
    "        print(f\"  Grid size: {[int(x) for x in grid_size_cp]}\")\n",
    "        print(f\"  Voxel size: {self.voxel_size}\")\n",
    "        \n",
    "        # Launch ray tracing kernel with fixed parameters\n",
    "        threads_per_block = 256\n",
    "        blocks = (self.num_lors + threads_per_block - 1) // threads_per_block\n",
    "        \n",
    "        self.ray_trace_kernel(\n",
    "            (blocks,), (threads_per_block,),\n",
    "            (lors_cp, grid_origin_cp, grid_size_cp, self.voxel_size,\n",
    "             voxel_indices, lor_indices, weights, counts, self.num_lors,\n",
    "             max_intersections_per_lor)\n",
    "        )\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        \n",
    "        # Compact results with debugging\n",
    "        total_intersections = int(cp.sum(counts))\n",
    "        print(f\"Total intersections: {total_intersections:,}\")\n",
    "        \n",
    "        if total_intersections == 0:\n",
    "            print(\"WARNING: No intersections found!\")\n",
    "            print(\"Debug info:\")\n",
    "            # Check a few LORs manually\n",
    "            sample_lors = lors_cp[:5]\n",
    "            for i, lor in enumerate(sample_lors):\n",
    "                x1, y1, z1, x2, y2, z2 = lor\n",
    "                print(f\"  LOR {i}: ({x1:.1f},{y1:.1f},{z1:.1f}) -> ({x2:.1f},{y2:.1f},{z2:.1f})\")\n",
    "                \n",
    "                # Convert to voxel coords manually\n",
    "                vx1 = (x1 - grid_origin_cp[0]) / self.voxel_size\n",
    "                vy1 = (y1 - grid_origin_cp[1]) / self.voxel_size  \n",
    "                vz1 = (z1 - grid_origin_cp[2]) / self.voxel_size\n",
    "                vx2 = (x2 - grid_origin_cp[0]) / self.voxel_size\n",
    "                vy2 = (y2 - grid_origin_cp[1]) / self.voxel_size\n",
    "                vz2 = (z2 - grid_origin_cp[2]) / self.voxel_size\n",
    "                print(f\"    Voxel coords: ({vx1:.1f},{vy1:.1f},{vz1:.1f}) -> ({vx2:.1f},{vy2:.1f},{vz2:.1f})\")\n",
    "                \n",
    "                # Check if any part is in bounds\n",
    "                in_bounds = ((0 <= vx1 < grid_size_cp[0] or 0 <= vx2 < grid_size_cp[0]) and\n",
    "                           (0 <= vy1 < grid_size_cp[1] or 0 <= vy2 < grid_size_cp[1]) and  \n",
    "                           (0 <= vz1 < grid_size_cp[2] or 0 <= vz2 < grid_size_cp[2]))\n",
    "                print(f\"    In bounds: {in_bounds}\")\n",
    "            \n",
    "            # Fallback to CPU ray tracing for comparison\n",
    "            print(\"Falling back to CPU ray tracing...\")\n",
    "            return self._build_system_matrix_cpu_fallback()\n",
    "        \n",
    "        # Create compact arrays\n",
    "        valid_voxels = []\n",
    "        valid_lors = []\n",
    "        valid_weights = []\n",
    "        \n",
    "        for lor_idx in range(self.num_lors):\n",
    "            count = int(counts[lor_idx])\n",
    "            if count > 0:\n",
    "                start_idx = lor_idx * max_intersections_per_lor\n",
    "                end_idx = start_idx + count\n",
    "                valid_voxels.extend(voxel_indices[start_idx:end_idx].tolist())\n",
    "                valid_lors.extend([lor_idx] * count)\n",
    "                valid_weights.extend(weights[start_idx:end_idx].tolist())\n",
    "        \n",
    "        # Validate and clean data before creating matrix\n",
    "        print(f\"Validating {len(valid_weights)} intersection weights...\")\n",
    "        valid_weights_clean = []\n",
    "        valid_voxels_clean = []\n",
    "        valid_lors_clean = []\n",
    "        \n",
    "        for i, (weight, voxel, lor) in enumerate(zip(valid_weights, valid_voxels, valid_lors)):\n",
    "            if np.isfinite(weight) and weight > 0 and 0 <= voxel < self.total_voxels:\n",
    "                valid_weights_clean.append(weight)\n",
    "                valid_voxels_clean.append(voxel)\n",
    "                valid_lors_clean.append(lor)\n",
    "        \n",
    "        print(f\"After cleaning: {len(valid_weights_clean)} valid intersections\")\n",
    "        \n",
    "        if len(valid_weights_clean) == 0:\n",
    "            print(\"ERROR: No valid intersections after cleaning!\")\n",
    "            # Create minimal dummy matrix\n",
    "            valid_weights_clean = [1.0]\n",
    "            valid_voxels_clean = [0]\n",
    "            valid_lors_clean = [0]\n",
    "        \n",
    "        # Create CSR matrix with cleaned data\n",
    "        row_indices = cp.array(valid_lors_clean, dtype=cp.int32)\n",
    "        col_indices = cp.array(valid_voxels_clean, dtype=cp.int32)\n",
    "        data = cp.array(valid_weights_clean, dtype=cp.float32)\n",
    "        \n",
    "        self.system_matrix_csr = cupy_csr_matrix(\n",
    "            (data, (row_indices, col_indices)), \n",
    "            shape=(self.num_lors, self.total_voxels)\n",
    "        )\n",
    "        \n",
    "        # Verify matrix was created properly\n",
    "        print(f\"System matrix created: shape={self.system_matrix_csr.shape}\")\n",
    "        print(f\"Data range: min={float(self.system_matrix_csr.data.min()):.6f}, \"\n",
    "              f\"max={float(self.system_matrix_csr.data.max()):.6f}\")\n",
    "        print(f\"NaN in data: {int(cp.sum(cp.isnan(self.system_matrix_csr.data)))}\")\n",
    "        \n",
    "        # Calculate sensitivity image (column sums) - GPU accelerated with fix\n",
    "        print(\"Calculating sensitivity image...\")\n",
    "        \n",
    "        # Method 1: Try direct sum\n",
    "        try:\n",
    "            sensitivity_raw = cp.array(self.system_matrix_csr.sum(axis=0)).flatten()\n",
    "            if cp.any(cp.isnan(sensitivity_raw)) or cp.any(cp.isinf(sensitivity_raw)):\n",
    "                print(\"WARNING: NaN/Inf in raw sensitivity - using alternative method\")\n",
    "                raise ValueError(\"Invalid sensitivity values\")\n",
    "            self.sensitivity_image = sensitivity_raw\n",
    "        except:\n",
    "            # Method 2: Manual calculation for robustness\n",
    "            print(\"Using manual sensitivity calculation...\")\n",
    "            self.sensitivity_image = cp.zeros(self.total_voxels, dtype=cp.float32)\n",
    "            \n",
    "            # Add weights manually - use cleaned data\n",
    "            for i in range(len(data)):\n",
    "                col_idx = int(col_indices[i])\n",
    "                weight = float(data[i])\n",
    "                if 0 <= col_idx < self.total_voxels:\n",
    "                    self.sensitivity_image[col_idx] += weight\n",
    "        \n",
    "        # Ensure no zeros or invalid values\n",
    "        self.sensitivity_image = cp.maximum(self.sensitivity_image, 1e-10)\n",
    "        self.sensitivity_image = cp.nan_to_num(self.sensitivity_image, nan=1e-10, posinf=1e10, neginf=1e-10)\n",
    "        \n",
    "        print(f\"Sensitivity image stats: min={float(self.sensitivity_image.min()):.6f}, \"\n",
    "              f\"max={float(self.sensitivity_image.max()):.6f}, \"\n",
    "              f\"mean={float(self.sensitivity_image.mean()):.6f}, \"\n",
    "              f\"zeros={int(cp.sum(self.sensitivity_image <= 1e-10))}\")\n",
    "        \n",
    "        print(f\"System matrix: {self.system_matrix_csr.shape}, \"\n",
    "              f\"{self.system_matrix_csr.nnz:,} non-zeros\")\n",
    "    \n",
    "    def _build_system_matrix_cpu_fallback(self):\n",
    "        \"\"\"CPU fallback when GPU ray tracing fails.\"\"\"\n",
    "        print(\"Building system matrix with CPU fallback...\")\n",
    "        \n",
    "        # Convert back to PyTorch for CPU processing\n",
    "        lors_torch = torch.as_tensor(self.lors)\n",
    "        \n",
    "        all_voxel_indices = []\n",
    "        all_lor_indices = []\n",
    "        all_weights = []\n",
    "        \n",
    "        # Process in batches\n",
    "        batch_size = 1000\n",
    "        num_batches = (self.num_lors + batch_size - 1) // batch_size\n",
    "        \n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min((batch_idx + 1) * batch_size, self.num_lors)\n",
    "            \n",
    "            lors_batch = lors_torch[start_idx:end_idx]\n",
    "            \n",
    "            # Simple ray tracing\n",
    "            for i, lor in enumerate(lors_batch):\n",
    "                x1, y1, z1, x2, y2, z2 = lor\n",
    "                \n",
    "                # Convert to voxel coordinates  \n",
    "                p1 = torch.tensor([(x1 - self.grid_origin[0]) / self.voxel_size,\n",
    "                                 (y1 - self.grid_origin[1]) / self.voxel_size,\n",
    "                                 (z1 - self.grid_origin[2]) / self.voxel_size])\n",
    "                p2 = torch.tensor([(x2 - self.grid_origin[0]) / self.voxel_size,\n",
    "                                 (y2 - self.grid_origin[1]) / self.voxel_size,\n",
    "                                 (z2 - self.grid_origin[2]) / self.voxel_size])\n",
    "                \n",
    "                # Simple sampling along ray\n",
    "                direction = p2 - p1\n",
    "                length = torch.norm(direction)\n",
    "                \n",
    "                if length > 1e-6:\n",
    "                    direction = direction / length\n",
    "                    \n",
    "                    # Sample points along ray\n",
    "                    num_samples = int(length * 2) + 10\n",
    "                    for step in range(num_samples):\n",
    "                        t = step / num_samples * length\n",
    "                        pos = p1 + t * direction\n",
    "                        \n",
    "                        vx, vy, vz = int(pos[0]), int(pos[1]), int(pos[2])\n",
    "                        \n",
    "                        if (0 <= vx < self.grid_size[0] and \n",
    "                            0 <= vy < self.grid_size[1] and\n",
    "                            0 <= vz < self.grid_size[2]):\n",
    "                            \n",
    "                            voxel_idx = vx * self.grid_size[1] * self.grid_size[2] + vy * self.grid_size[2] + vz\n",
    "                            lor_idx = start_idx + i\n",
    "                            weight = length / num_samples * self.voxel_size\n",
    "                            \n",
    "                            all_voxel_indices.append(int(voxel_idx))\n",
    "                            all_lor_indices.append(int(lor_idx))\n",
    "                            all_weights.append(float(weight))\n",
    "            \n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f\"CPU fallback: batch {batch_idx + 1}/{num_batches}\")\n",
    "        \n",
    "        if len(all_voxel_indices) == 0:\n",
    "            print(\"ERROR: Even CPU fallback found no intersections!\")\n",
    "            # Create minimal dummy matrix to avoid crashes\n",
    "            all_voxel_indices = [0]\n",
    "            all_lor_indices = [0] \n",
    "            all_weights = [1e-10]\n",
    "        \n",
    "        # Create CSR matrix\n",
    "        row_indices = cp.array(all_lor_indices, dtype=cp.int32)\n",
    "        col_indices = cp.array(all_voxel_indices, dtype=cp.int32)\n",
    "        data = cp.array(all_weights, dtype=cp.float32)\n",
    "        \n",
    "        self.system_matrix_csr = cupy_csr_matrix(\n",
    "            (data, (row_indices, col_indices)), \n",
    "            shape=(self.num_lors, self.total_voxels)\n",
    "        )\n",
    "        \n",
    "        # Calculate sensitivity image\n",
    "        self.sensitivity_image = cp.array(self.system_matrix_csr.sum(axis=0)).flatten()\n",
    "        self.sensitivity_image[self.sensitivity_image == 0] = 1.0\n",
    "    \n",
    "    def forward_project_gpu(self, image_cp: cp.ndarray) -> cp.ndarray:\n",
    "        \"\"\"Ultra-fast GPU forward projection using custom CUDA kernel.\"\"\"\n",
    "        sino = cp.zeros(self.num_lors, dtype=cp.float32)\n",
    "        \n",
    "        threads_per_block = 256\n",
    "        blocks = (self.num_lors + threads_per_block - 1) // threads_per_block\n",
    "        \n",
    "        self.forward_proj_kernel(\n",
    "            (blocks,), (threads_per_block,),\n",
    "            (image_cp, self.system_matrix_csr.indptr, \n",
    "             self.system_matrix_csr.indices, self.system_matrix_csr.data,\n",
    "             sino, self.num_lors)\n",
    "        )\n",
    "        \n",
    "        return sino\n",
    "    \n",
    "    def back_project_gpu(self, sino_cp: cp.ndarray) -> cp.ndarray:\n",
    "        \"\"\"Ultra-fast GPU back projection using custom CUDA kernel.\"\"\"\n",
    "        image = cp.zeros(self.total_voxels, dtype=cp.float32)\n",
    "        \n",
    "        threads_per_block = 256\n",
    "        blocks = (self.num_lors + threads_per_block - 1) // threads_per_block\n",
    "        \n",
    "        self.back_proj_kernel(\n",
    "            (blocks,), (threads_per_block,),\n",
    "            (sino_cp, self.system_matrix_csr.indptr,\n",
    "             self.system_matrix_csr.indices, self.system_matrix_csr.data,\n",
    "             image, self.num_lors)\n",
    "        )\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def estimate_scatter_gpu(self, image_cp: cp.ndarray, scatter_fraction: float = 0.15) -> cp.ndarray:\n",
    "        \"\"\"GPU-accelerated scatter estimation with 3D convolution.\"\"\"\n",
    "        # Reshape to 3D\n",
    "        img_3d = image_cp.reshape(self.grid_size[0].item(), \n",
    "                                 self.grid_size[1].item(), \n",
    "                                 self.grid_size[2].item())\n",
    "        \n",
    "        # Fast 3D Gaussian convolution using separable filters\n",
    "        sigma = 2.0\n",
    "        kernel_size = 7\n",
    "        \n",
    "        # Create 1D Gaussian kernel\n",
    "        x = cp.arange(kernel_size) - kernel_size // 2\n",
    "        kernel_1d = cp.exp(-x**2 / (2 * sigma**2))\n",
    "        kernel_1d /= kernel_1d.sum()\n",
    "        \n",
    "        # Apply separable 3D convolution (much faster than full 3D)\n",
    "        from cupyx.scipy.ndimage import convolve1d\n",
    "        \n",
    "        # Convolve along each axis\n",
    "        scattered = convolve1d(img_3d, kernel_1d, axis=0, mode='constant')\n",
    "        scattered = convolve1d(scattered, kernel_1d, axis=1, mode='constant')\n",
    "        scattered = convolve1d(scattered, kernel_1d, axis=2, mode='constant')\n",
    "        \n",
    "        # Forward project and scale\n",
    "        scattered_flat = scattered.flatten()\n",
    "        scatter_sino = self.forward_project_gpu(scattered_flat)\n",
    "        \n",
    "        return scatter_sino * scatter_fraction\n",
    "    \n",
    "    def reconstruct_gpu(self, measured_data: torch.Tensor, \n",
    "                       attenuation_mask: torch.Tensor,\n",
    "                       num_iterations: int = 20, \n",
    "                       convergence_threshold: float = 1e-4) -> torch.Tensor:\n",
    "        \"\"\"Ultra-fast GPU MLEM reconstruction with numerical stability.\"\"\"\n",
    "        print(\"Starting optimized GPU MLEM reconstruction...\")\n",
    "        \n",
    "        # Build system matrix if needed\n",
    "        if self.system_matrix_csr is None:\n",
    "            self.build_system_matrix_gpu()\n",
    "        \n",
    "        # Convert to CuPy for maximum speed\n",
    "        measured_cp = cp.asarray(measured_data.to(self.device), dtype=cp.float32)\n",
    "        atten_cp = cp.asarray(attenuation_mask.to(self.device).flatten(), dtype=cp.float32)\n",
    "        \n",
    "        # Debug system matrix data quality\n",
    "        print(f\"System matrix data check:\")\n",
    "        data_stats = self.system_matrix_csr.data\n",
    "        print(f\"  Data stats: min={float(data_stats.min()):.6f}, max={float(data_stats.max()):.6f}\")\n",
    "        print(f\"  NaN count: {int(cp.sum(cp.isnan(data_stats)))}\")\n",
    "        print(f\"  Inf count: {int(cp.sum(cp.isinf(data_stats)))}\")\n",
    "        print(f\"  Zero count: {int(cp.sum(data_stats == 0))}\")\n",
    "        \n",
    "        # Clean system matrix data\n",
    "        if cp.any(cp.isnan(data_stats)) or cp.any(cp.isinf(data_stats)):\n",
    "            print(\"Cleaning system matrix data...\")\n",
    "            self.system_matrix_csr.data = cp.nan_to_num(data_stats, nan=0.0, posinf=1e6, neginf=0.0)\n",
    "            # Remove zeros after cleaning\n",
    "            self.system_matrix_csr.eliminate_zeros()\n",
    "        \n",
    "        # Debug system matrix\n",
    "        print(f\"System matrix nnz: {self.system_matrix_csr.nnz}\")\n",
    "        print(f\"Sensitivity image stats: min={float(self.sensitivity_image.min()):.6f}, \"\n",
    "              f\"max={float(self.sensitivity_image.max()):.6f}, \"\n",
    "              f\"mean={float(self.sensitivity_image.mean()):.6f}\")\n",
    "        \n",
    "        # Check for zeros in sensitivity\n",
    "        zero_sens = cp.sum(self.sensitivity_image <= 1e-10)\n",
    "        if zero_sens > 0:\n",
    "            print(f\"WARNING: {zero_sens} voxels have near-zero sensitivity\")\n",
    "            \n",
    "        # More robust sensitivity handling\n",
    "        sens_mean = float(self.sensitivity_image.mean())\n",
    "        sens_threshold = max(1e-10, sens_mean * 1e-6)  # Adaptive threshold\n",
    "        self.sensitivity_image = cp.maximum(self.sensitivity_image, sens_threshold)\n",
    "        \n",
    "        print(f\"Applied sensitivity threshold: {sens_threshold:.2e}\")\n",
    "        print(f\"Updated sensitivity stats: min={float(self.sensitivity_image.min()):.6f}, \"\n",
    "              f\"max={float(self.sensitivity_image.max()):.6f}\")\n",
    "        \n",
    "        # Initialize image with better scaling\n",
    "        total_sensitivity = float(cp.sum(self.sensitivity_image))\n",
    "        image_cp = cp.ones(self.total_voxels, dtype=cp.float32)\n",
    "        total_measured = float(cp.sum(measured_cp))\n",
    "        if total_measured > 0 and total_sensitivity > 0:\n",
    "            # Scale by ratio of measured counts to total sensitivity\n",
    "            scale_factor = total_measured / total_sensitivity\n",
    "            image_cp *= scale_factor\n",
    "            print(f\"Image initialization scale factor: {scale_factor:.2e}\")\n",
    "        \n",
    "        print(f\"Initial image stats: min={float(image_cp.min()):.3e}, \"\n",
    "              f\"max={float(image_cp.max()):.3e}, mean={float(image_cp.mean()):.3e}\")\n",
    "        \n",
    "        print(f\"Measured data stats: sum={total_measured:.1f}, \"\n",
    "              f\"min={float(measured_cp.min()):.3f}, max={float(measured_cp.max()):.3f}\")\n",
    "        \n",
    "        # Apply attenuation correction more carefully\n",
    "        print(\"Applying attenuation correction...\")\n",
    "        try:\n",
    "            atten_factors = self.forward_project_gpu(atten_cp)\n",
    "            \n",
    "            # Check attenuation factors\n",
    "            print(f\"Attenuation factors: min={float(atten_factors.min()):.3f}, \"\n",
    "                  f\"max={float(atten_factors.max()):.3f}, \"\n",
    "                  f\"mean={float(atten_factors.mean()):.3f}\")\n",
    "            \n",
    "            if cp.any(cp.isnan(atten_factors)) or cp.any(cp.isinf(atten_factors)):\n",
    "                print(\"WARNING: Invalid attenuation factors detected\")\n",
    "                atten_factors = cp.nan_to_num(atten_factors, nan=0.1, posinf=10, neginf=0)\n",
    "            \n",
    "            atten_factors = cp.clip(atten_factors, 0, 10)  # Reasonable attenuation range\n",
    "            atten_correction = cp.exp(-atten_factors)\n",
    "            atten_correction = cp.clip(atten_correction, 1e-6, 1.0)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Attenuation correction failed: {e}, using no correction\")\n",
    "            atten_correction = cp.ones(self.num_lors, dtype=cp.float32)\n",
    "        \n",
    "        measured_corrected = measured_cp * atten_correction\n",
    "        \n",
    "        print(f\"Attenuation correction stats: min={float(atten_correction.min()):.6f}, \"\n",
    "              f\"max={float(atten_correction.max()):.6f}\")\n",
    "        print(f\"Corrected data sum: {float(cp.sum(measured_corrected)):.1f}\")\n",
    "        \n",
    "        # Pre-allocate arrays\n",
    "        estimated_sino = cp.zeros(self.num_lors, dtype=cp.float32)\n",
    "        correction = cp.zeros(self.total_voxels, dtype=cp.float32)\n",
    "        \n",
    "        # Initialize corrections\n",
    "        scatter = cp.zeros(self.num_lors, dtype=cp.float32)\n",
    "        randoms = cp.full(self.num_lors, max(0.01, total_measured * 0.001), dtype=cp.float32)\n",
    "        \n",
    "        prev_likelihood = float('inf')\n",
    "        \n",
    "        # MLEM iterations with enhanced stability\n",
    "        for iteration in range(num_iterations):\n",
    "            # Forward project with stability check\n",
    "            estimated_sino = self.forward_project_gpu(image_cp)\n",
    "            \n",
    "            # Check for issues\n",
    "            if cp.any(cp.isnan(estimated_sino)) or cp.any(cp.isinf(estimated_sino)):\n",
    "                print(f\"WARNING: NaN/Inf in forward projection at iteration {iteration + 1}\")\n",
    "                estimated_sino = cp.nan_to_num(estimated_sino, nan=1e-10, posinf=1e10, neginf=1e-10)\n",
    "            \n",
    "            # Add corrections with stability\n",
    "            if iteration % 3 == 0:  # Update scatter less frequently for stability\n",
    "                try:\n",
    "                    scatter = self.estimate_scatter_gpu(image_cp)\n",
    "                    scatter = cp.clip(scatter, 0, total_measured * 0.5)  # Limit scatter\n",
    "                except:\n",
    "                    print(\"Scatter estimation failed, using previous values\")\n",
    "            \n",
    "            total_estimated = estimated_sino + scatter + randoms\n",
    "            total_estimated = cp.clip(total_estimated, 1e-8, 1e10)  # Robust clipping\n",
    "            \n",
    "            # Check estimated data\n",
    "            if cp.any(cp.isnan(total_estimated)):\n",
    "                print(f\"WARNING: NaN in total_estimated at iteration {iteration + 1}\")\n",
    "                total_estimated = cp.nan_to_num(total_estimated, nan=1e-8)\n",
    "            \n",
    "            # Calculate ratio with stability\n",
    "            ratio = measured_corrected / total_estimated\n",
    "            ratio = cp.clip(ratio, 0, 100)  # Limit extreme ratios\n",
    "            ratio = cp.nan_to_num(ratio, nan=1.0, posinf=1.0, neginf=0.0)\n",
    "            \n",
    "            # Back project with stability check\n",
    "            correction = self.back_project_gpu(ratio)\n",
    "            \n",
    "            if cp.any(cp.isnan(correction)) or cp.any(cp.isinf(correction)):\n",
    "                print(f\"WARNING: NaN/Inf in back projection at iteration {iteration + 1}\")\n",
    "                correction = cp.nan_to_num(correction, nan=1.0, posinf=1.0, neginf=0.0)\n",
    "            \n",
    "            # MLEM update with enhanced stability\n",
    "            update_factor = correction / self.sensitivity_image\n",
    "            update_factor = cp.clip(update_factor, 0.1, 10.0)  # Limit update magnitude\n",
    "            update_factor = cp.nan_to_num(update_factor, nan=1.0)\n",
    "            \n",
    "            image_cp *= update_factor\n",
    "            image_cp = cp.clip(image_cp, 1e-10, 1e6)  # Robust non-negativity and upper bound\n",
    "            \n",
    "            # Stability check on image\n",
    "            if cp.any(cp.isnan(image_cp)) or cp.any(cp.isinf(image_cp)):\n",
    "                print(f\"ERROR: NaN/Inf in image at iteration {iteration + 1}\")\n",
    "                image_cp = cp.nan_to_num(image_cp, nan=1e-10, posinf=1e6, neginf=1e-10)\n",
    "            \n",
    "            # Robust likelihood calculation\n",
    "            if iteration % 2 == 0:\n",
    "                try:\n",
    "                    # Ensure positive values for log\n",
    "                    log_arg = cp.clip(total_estimated, 1e-10, 1e10)\n",
    "                    likelihood = float(cp.sum(measured_corrected * cp.log(log_arg) - total_estimated))\n",
    "                    \n",
    "                    if not cp.isfinite(likelihood):\n",
    "                        likelihood = -1e10  # Fallback for numerical issues\n",
    "                    \n",
    "                    if iteration > 0 and cp.isfinite(prev_likelihood):\n",
    "                        rel_change = abs(likelihood - prev_likelihood) / (abs(prev_likelihood) + 1e-10)\n",
    "                        print(f\"Iteration {iteration + 1}: Log-likelihood = {likelihood:.2f}, \"\n",
    "                              f\"Rel. change = {rel_change:.6f}\")\n",
    "                        \n",
    "                        # Check image stats\n",
    "                        img_stats = f\"Image: min={float(image_cp.min()):.3e}, max={float(image_cp.max()):.3e}, mean={float(image_cp.mean()):.3e}\"\n",
    "                        print(f\"  {img_stats}\")\n",
    "                        \n",
    "                        if rel_change < convergence_threshold:\n",
    "                            print(f\"Converged after {iteration + 1} iterations\")\n",
    "                            break\n",
    "                    else:\n",
    "                        print(f\"Iteration {iteration + 1}: Log-likelihood = {likelihood:.2f}\")\n",
    "                    \n",
    "                    prev_likelihood = likelihood\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Likelihood calculation failed: {e}\")\n",
    "                    likelihood = -1e10\n",
    "            else:\n",
    "                print(f\"Iteration {iteration + 1}: (likelihood check skipped)\")\n",
    "        \n",
    "        # Final stability check\n",
    "        image_cp = cp.nan_to_num(image_cp, nan=0.0, posinf=1e6, neginf=0.0)\n",
    "        image_cp = cp.clip(image_cp, 0, 1e6)\n",
    "        \n",
    "        # Convert back to PyTorch\n",
    "        result = torch.as_tensor(image_cp, device=self.device)\n",
    "        return result\n",
    "    \n",
    "    def get_image_3d(self, image_1d: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Convert 1D image to 3D array.\"\"\"\n",
    "        return image_1d.view(self.grid_size[0], self.grid_size[1], self.grid_size[2])\n",
    "\n",
    "# Optimized example usage\n",
    "def optimized_example():\n",
    "    \"\"\"Example with GPU optimization features.\"\"\"\n",
    "    \n",
    "    # Enable GPU optimizations\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Generate test data\n",
    "    num_lors = 100000  # Larger dataset to show performance gains\n",
    "    lors = torch.randn(num_lors, 6, device='cuda') * 100\n",
    "    measured_counts = torch.poisson(torch.ones(num_lors, device='cuda') * 50)\n",
    "    \n",
    "    # Initialize optimized reconstructor\n",
    "    reconstructor = OptimizedCUDAMLEM(lors, voxel_size=2.0, device='cuda')\n",
    "    \n",
    "    # Create attenuation mask\n",
    "    grid_shape = reconstructor.grid_size\n",
    "    attenuation_mask = torch.ones(grid_shape[0], grid_shape[1], grid_shape[2], \n",
    "                                device='cuda') * 0.1\n",
    "    \n",
    "    # Time the reconstruction\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Perform optimized reconstruction\n",
    "    reconstructed_image = reconstructor.reconstruct_gpu(\n",
    "        measured_counts, \n",
    "        attenuation_mask,\n",
    "        num_iterations=10\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nReconstruction completed in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Convert to 3D\n",
    "    image_3d = reconstructor.get_image_3d(reconstructed_image)\n",
    "    \n",
    "    print(f\"Final image shape: {image_3d.shape}\")\n",
    "    print(f\"Image statistics: min={image_3d.min():.3f}, max={image_3d.max():.3f}, \"\n",
    "          f\"mean={image_3d.mean():.3f}\")\n",
    "    \n",
    "    return reconstructed_image, image_3d\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    optimized_example()\n",
    "\n",
    "\n",
    "# # Optimized example usage\n",
    "# def optimized_example():\n",
    "#     \"\"\"Example with GPU optimization features.\"\"\"\n",
    "    \n",
    "#     # Enable GPU optimizations\n",
    "#     torch.backends.cudnn.benchmark = True\n",
    "#     torch.cuda.empty_cache()\n",
    "#     # Generate test data\n",
    "#     num_lors = filtered_coordinates.shape[0]  # Use existing LOR count\n",
    "#     lors = filtered_coordinates.clone().to('cuda')  # Use filtered coordinates directly\n",
    "#     measured_counts = torch.poisson(torch.ones(num_lors, device='cuda') * 50)  # Fix ones() argument\n",
    "#     # Initialize optimized reconstructor\n",
    "#     reconstructor = OptimizedCUDAMLEM(lors, voxel_size=2.0, device='cuda')\n",
    "    \n",
    "#     # Create attenuation mask\n",
    "#     grid_shape = reconstructor.grid_size\n",
    "#     attenuation_mask = torch.ones(grid_shape[0], grid_shape[1], grid_shape[2], \n",
    "#                                 device='cuda') * 0.1\n",
    "    \n",
    "#     # Time the reconstruction\n",
    "#     import time\n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     # Perform optimized reconstruction\n",
    "#     reconstructed_image = reconstructor.reconstruct_gpu(\n",
    "#         measured_counts, \n",
    "#         attenuation_mask,\n",
    "#         num_iterations=100\n",
    "#     )\n",
    "    \n",
    "#     end_time = time.time()\n",
    "#     print(f\"\\nReconstruction completed in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "#     # Convert to 3D\n",
    "#     image_3d = reconstructor.get_image_3d(reconstructed_image)\n",
    "    \n",
    "#     print(f\"Final image shape: {image_3d.shape}\")\n",
    "#     print(f\"Image statistics: min={image_3d.min():.3f}, max={image_3d.max():.3f}, \"\n",
    "#           f\"mean={image_3d.mean():.3f}\")\n",
    "    \n",
    "#     # Convert tensor to numpy for plotting\n",
    "#     recon_np = image3d.cpu().numpy()\n",
    "#     nx, ny, nz = recon_np.shape\n",
    "\n",
    "#     def plot_cross_sections_horizontal(x_idx=nx//2, y_idx=ny//2, z_idx=nz//2, vmax=0.5, cmap='Magma'):\n",
    "#         fig = make_subplots(rows=1, cols=3, subplot_titles=[\n",
    "#             f'XY plane @ z={z_idx}',\n",
    "#             f'XZ plane @ y={y_idx}',\n",
    "#             f'YZ plane @ x={x_idx}'\n",
    "#         ])\n",
    "\n",
    "#         # XY plane at z=z_idx\n",
    "#         fig.add_trace(go.Heatmap(\n",
    "#             z=recon_np[:, :, z_idx].T,\n",
    "#             colorscale=cmap,\n",
    "#             zmax=vmax,\n",
    "#             zmin=0,\n",
    "#             showscale=True,\n",
    "#             name=f'XY @ z={z_idx}'\n",
    "#         ), row=1, col=1)\n",
    "\n",
    "#         # XZ plane at y=y_idx\n",
    "#         fig.add_trace(go.Heatmap(\n",
    "#             z=recon_np[:, y_idx, :].T,\n",
    "#             colorscale=cmap,\n",
    "#             zmax=vmax,\n",
    "#             zmin=0,\n",
    "#             showscale=True,\n",
    "#             name=f'XZ @ y={y_idx}'\n",
    "#         ), row=1, col=2)\n",
    "\n",
    "#         # YZ plane at x=x_idx\n",
    "#         fig.add_trace(go.Heatmap(\n",
    "#             z=recon_np[x_idx, :, :].T,\n",
    "#             colorscale=cmap,\n",
    "#             zmax=vmax,\n",
    "#             zmin=0,\n",
    "#             showscale=True,\n",
    "#             name=f'YZ @ x={x_idx}'\n",
    "#         ), row=1, col=3)\n",
    "\n",
    "#         fig.update_layout(\n",
    "#             width=1200,\n",
    "#             height=400,\n",
    "#             title_text=\"Orthogonal Cross Sections\"\n",
    "#         )\n",
    "#         fig.show()\n",
    "\n",
    "#     interact(\n",
    "#         plot_cross_sections_horizontal,\n",
    "#         x_idx=IntSlider(min=0, max=nx-1, step=1, value=nx//2, description='X index'),\n",
    "#         y_idx=IntSlider(min=0, max=ny-1, step=1, value=ny//2, description='Y index'),\n",
    "#         z_idx=IntSlider(min=0, max=nz-1, step=1, value=nz//2, description='Z index'),\n",
    "#         vmax=FloatSlider(min=0, max=1, step=0.01, value=0.05, description='vmax'),\n",
    "#         cmap=['Magma','Greys', 'Viridis', 'Cividis', 'Plasma'])\n",
    "    \n",
    "#     return reconstructed_image, image_3d\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     optimized_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a708d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
