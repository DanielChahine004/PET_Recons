import torch
import torch.nn as nn
import torch.nn.functional as F
import time
from typing import Tuple

###############################################################################
# TorchScript-Compatible Global Attention Module with Skip Connection
###############################################################################
class GlobalAttention3D(nn.Module):
    """
    Fully TorchScript-compatible global attention module for 3D feature maps with skip connection.
    """
    
    def __init__(self, in_channels: int = 64, embed_dim: int = 128, output_dim: int = 64, 
                 num_heads: int = 2, max_seq_len: int = 1000):
        super(GlobalAttention3D, self).__init__()
        
        self.in_channels = in_channels
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.output_dim = output_dim
        self.max_seq_len = max_seq_len
        
        assert embed_dim % num_heads == 0, "embed_dim must be divisible by num_heads"
        
        # Map from input channels to embedding dimension
        self.channel_proj = nn.Linear(in_channels, embed_dim)
        
        # Manual attention projections
        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False)
        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)
        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False)
        self.out_proj = nn.Linear(embed_dim, embed_dim)
        
        # Map from embedding dimension to output channels
        self.output_proj = nn.Linear(embed_dim, output_dim)
        
        # Pre-allocated positional encoding (TorchScript compatible)
        self.register_buffer('pos_encoding', torch.randn(1, max_seq_len, embed_dim) * 0.02)
        self.scale = self.head_dim ** -0.5
        
        # Always create skip projection modules (TorchScript compatible)
        self.skip_proj = nn.Conv3d(in_channels, output_dim, kernel_size=1, bias=False)
        self.skip_bn = nn.BatchNorm3d(output_dim)
        self.use_skip_proj = (in_channels != output_dim)
        
        # Learnable gate for skip connection strength
        self.gate = nn.Parameter(torch.tensor(0.1))
        
    def forward(self, x: torch.Tensor, debug: bool = False) -> torch.Tensor:
        """
        x: (batch_size, channels, D, H, W)
        Returns: (batch_size, output_dim, D, H, W)
        """
        batch_size, channels, D, H, W = x.shape
        seq_len = D * H * W
        
        if debug: print(f"GlobalAttention input: {x.shape}")
        
        # Store input for skip connection
        skip_input = x
        
        # Reshape to treat spatial positions as sequence tokens
        x_flat = x.permute(0, 2, 3, 4, 1).contiguous()
        x_flat = x_flat.view(batch_size, seq_len, channels)
        if debug: print(f"After reshaping to sequence: {x_flat.shape}")
        
        # Project to embedding dimension
        x_flat = self.channel_proj(x_flat)
        if debug: print(f"After channel projection: {x_flat.shape}")
        
        # Add positional encoding (slice to current sequence length)
        x_flat = x_flat + self.pos_encoding[:, :seq_len, :]
        if debug: print(f"After positional encoding: {x_flat.shape}")
        
        # Manual multi-head attention
        q = self.q_proj(x_flat)
        k = self.k_proj(x_flat)
        v = self.v_proj(x_flat)
        
        # Reshape for multi-head attention
        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        if debug: print(f"Q, K, V shapes after head reshaping: {q.shape}, {k.shape}, {v.shape}")
        
        # Scaled dot-product attention
        attn_weights = torch.matmul(q, k.transpose(-2, -1)) * self.scale
        attn_weights = torch.softmax(attn_weights, dim=-1)
        
        # Apply attention to values
        attn_output = torch.matmul(attn_weights, v)
        
        # Concatenate heads
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)
        if debug: print(f"After concatenating heads: {attn_output.shape}")
        
        # Output projection
        attn_output = self.out_proj(attn_output)
        
        # Project to output channels per token
        output = self.output_proj(attn_output)
        if debug: print(f"After output projection: {output.shape}")
        
        # Reshape back to conv format
        output = output.permute(0, 2, 1)
        output = output.view(batch_size, self.output_dim, D, H, W)
        if debug: print(f"After reshaping back to 3D: {output.shape}")
        
        # Skip connection (always apply projection, but conditionally use it)
        projected_skip = self.skip_bn(self.skip_proj(skip_input))
        if self.use_skip_proj:
            skip_input = projected_skip
        
        # Add skip connection with learnable gate
        output = output + self.gate * skip_input
        if debug: print(f"After skip connection: {output.shape}")
        
        return output


###############################################################################
# TorchScript-Compatible Normalized Tensor Train Module
###############################################################################
class NormalizedDirectTensorTrain3D(nn.Module):
    """
    TorchScript-compatible direct tensor train with pre-normalization for stability.
    """
    
    def __init__(self, input_shape: Tuple[int, int, int, int], rank: int = 64, norm_type: int = 0):
        super(NormalizedDirectTensorTrain3D, self).__init__()
        
        self.channels, self.D, self.H, self.W = input_shape
        self.rank = rank
        self.norm_type = norm_type
        
        # Always create all normalization types (TorchScript compatible)
        self.batch_norm = nn.BatchNorm3d(self.channels)
        self.layer_norm = nn.LayerNorm([self.D, self.H, self.W])
        num_groups = min(8, self.channels)
        self.group_norm = nn.GroupNorm(num_groups, self.channels)
        self.instance_norm = nn.InstanceNorm3d(self.channels)
        self.identity = nn.Identity()
        
        # TT cores with improved initialization
        self.core1 = nn.Parameter(torch.randn(1, self.D, rank) * (2.0 / (self.D + rank)) ** 0.5)
        self.core2 = nn.Parameter(torch.randn(rank, self.H, rank) * (2.0 / (self.H + 2*rank)) ** 0.5)
        self.core3 = nn.Parameter(torch.randn(rank, self.W, 1) * (2.0 / (self.W + rank)) ** 0.5)
        
        # Activation function for output stabilization
        self.activation = nn.GELU()
        
        self.output_size = rank * 3
        
    def forward(self, x: torch.Tensor, debug: bool = False) -> torch.Tensor:
        """
        Apply normalization then tensor train decomposition.
        """
        batch_size, channels, D, H, W = x.shape
        
        if debug: print(f"TensorTrain input: {x.shape}")
        
        # Apply normalization based on norm_type (TorchScript compatible)
        if self.norm_type == 0:  # 'batch'
            x_norm = self.batch_norm(x)
        elif self.norm_type == 1:  # 'layer'
            x_norm = self.layer_norm(x)
        elif self.norm_type == 2:  # 'group'
            x_norm = self.group_norm(x)
        elif self.norm_type == 3:  # 'instance'
            x_norm = self.instance_norm(x)
        else:  # 'none'
            x_norm = self.identity(x)
        
        if debug: print(f"After normalization: {x_norm.shape}")
        
        # Reshape for processing
        x_flat = x_norm.reshape(batch_size * channels, D, H, W)
        if debug: print(f"Flattened for TT processing: {x_flat.shape}")
        
        # Mode-wise contractions with improved numerical stability
        # Mode-1: contract over D dimension
        mode1_features = torch.einsum('bdhw,idr->bhwr', x_flat, self.core1)
        mode1_summary = torch.mean(mode1_features, dim=(1, 2))  # (B*C, rank)
        
        # Mode-2: contract over H dimension  
        mode2_features = torch.einsum('bdhw,rhs->bdwrs', x_flat, self.core2)
        mode2_summary = torch.mean(mode2_features, dim=(1, 2, 4))  # (B*C, rank)
        
        # Mode-3: contract over W dimension
        mode3_features = torch.einsum('bdhw,rwi->bdhr', x_flat, self.core3)
        mode3_summary = torch.mean(mode3_features, dim=(1, 2))  # (B*C, rank)
        
        # Concatenate mode summaries
        core_summaries = torch.cat([mode1_summary, mode2_summary, mode3_summary], dim=1)
        if debug: print(f"Concatenated core summaries: {core_summaries.shape}")
        
        # Apply activation function for stabilization and non-linearity
        core_summaries = self.activation(core_summaries)
        
        # Reshape back to batch format
        output = core_summaries.reshape(batch_size, channels * self.output_size)
        if debug: print(f"TensorTrain output: {output.shape}")
        
        return output


###############################################################################
# TorchScript-Compatible 3D Residual Block
###############################################################################
class ResidualBlock3D(nn.Module):
    """
    TorchScript-compatible 3D adaptation of a 2-layer residual block.
    """

    def __init__(self, in_channels: int, out_channels: int, stride: Tuple[int, int, int] = (1, 2, 2)):
        super(ResidualBlock3D, self).__init__()

        # First 3D conv
        self.conv1 = nn.Conv3d(in_channels, out_channels,
                               kernel_size=3,
                               stride=stride,
                               padding=0,
                               bias=False)
        self.bn1 = nn.BatchNorm3d(out_channels)

        # Second 3D conv
        self.conv2 = nn.Conv3d(out_channels, out_channels,
                               kernel_size=3,
                               stride=1,
                               padding=0,
                               bias=False)
        self.bn2 = nn.BatchNorm3d(out_channels)

        # Post skip-connection processing layer
        self.conv_post = nn.Conv3d(out_channels, out_channels,
                                   kernel_size=3,
                                   stride=1,
                                   padding=0,
                                   bias=False)
        self.bn_post = nn.BatchNorm3d(out_channels)

        self.activation = nn.GELU()

        # Always create shortcut modules (TorchScript compatible)
        self.shortcut_conv = nn.Conv3d(in_channels, out_channels,
                                      kernel_size=1,
                                      stride=stride,
                                      bias=False)
        self.shortcut_bn = nn.BatchNorm3d(out_channels)
        self.use_shortcut = (stride != (1, 1, 1) or in_channels != out_channels)

    def _apply_circular_padding(self, x: torch.Tensor) -> torch.Tensor:
        """
        Apply circular padding for width axis and regular padding for time and height.
        """
        # Circular padding for width, constant for others
        x = torch.nn.functional.pad(x, (1, 1, 0, 0, 0, 0), mode='circular')
        x = torch.nn.functional.pad(x, (0, 0, 1, 1, 1, 1), mode='constant', value=0.0)
        return x

    def forward(self, x: torch.Tensor, debug: bool = False) -> torch.Tensor:
        if debug: print(f"ResBlock input: {x.shape}")
        
        # Main path
        out = self._apply_circular_padding(x)
        if debug: print(f"After padding: {out.shape}")
        out = self.conv1(out)
        out = self.bn1(out)
        out = self.activation(out)
        if debug: print(f"After conv1+bn1+activation: {out.shape}")

        out = self._apply_circular_padding(out)
        out = self.conv2(out)
        out = self.bn2(out)
        if debug: print(f"After conv2+bn2: {out.shape}")

        # Shortcut (always compute, conditionally use)
        shortcut_out = self.shortcut_bn(self.shortcut_conv(x))
        if self.use_shortcut:
            x = shortcut_out
        if debug: print(f"Shortcut: {x.shape}")

        # Residual connection
        out += x
        if debug: print(f"After residual connection: {out.shape}")

        # Post skip-connection processing
        out = self._apply_circular_padding(out)
        out = self.conv_post(out)
        out = self.bn_post(out)
        out = self.activation(out)
        if debug: print(f"After post-processing: {out.shape}")
        
        return out


###############################################################################
# TorchScript-Compatible PetNetImproved3D
###############################################################################
class PetNetImproved3D(nn.Module):
    """
    Fully TorchScript-compatible PetNet with static tensor train initialization.
    """

    def __init__(self, num_classes: int = 6, tt_rank: int = 32, norm_type: int = 0, 
                 input_shape: Tuple[int, int, int, int] = (2, 3, 207, 41)):
        super(PetNetImproved3D, self).__init__()
        print("Loading PetNetImproved3D Model...")

        self.tt_rank = tt_rank
        self.norm_type = norm_type
        self.input_shape = input_shape

        # Initial 3D conv: 2 => 16 channels
        self.conv_in = nn.Conv3d(in_channels=2, out_channels=16,
                                 kernel_size=3, stride=(1, 1, 1),
                                 padding=1, bias=False)
        self.bn_in = nn.BatchNorm3d(16)
        self.activation = nn.GELU()

        # Residual blocks
        self.layer1 = ResidualBlock3D(16, 32, stride=(1, 2, 2))
        self.layer2 = ResidualBlock3D(32, 64, stride=(1, 2, 2))

        # Global attention after layer2
        # Calculate max_seq_len based on worst-case scenario after layer2
        max_seq_len = self._estimate_max_seq_len(input_shape)
        self.global_attention = GlobalAttention3D(
            in_channels=64, 
            embed_dim=128, 
            output_dim=64,
            num_heads=8,
            max_seq_len=max_seq_len
        )

        # Remaining residual blocks
        self.layer3 = ResidualBlock3D(64, 128, stride=(1, 2, 2))
        self.layer4 = ResidualBlock3D(128, 256, stride=(1, 2, 2))

        # **STATIC TENSOR TRAIN INITIALIZATION**
        feature_shape = self._compute_feature_shape(input_shape)
        self.tensor_train = NormalizedDirectTensorTrain3D(
            feature_shape, 
            rank=self.tt_rank,
            norm_type=self.norm_type
        )
        
        self.dropout = nn.Dropout(0.3)

        # Compute FC input features
        fc_in_features = feature_shape[0] * self.tensor_train.output_size
        self.fc1 = nn.Linear(fc_in_features, 1024, bias=True)
        self.fc2 = nn.Linear(1024, num_classes, bias=True)

        self._initialize_weights()

    def _estimate_max_seq_len(self, input_shape: Tuple[int, int, int, int]) -> int:
        """
        Estimate maximum sequence length for positional encoding buffer.
        """
        C, T, H, W = input_shape
        # Conservative estimate: assume minimal downsampling
        # After 2 layers with stride (1,2,2), roughly: T, H//4, W//4
        max_t = T + 10  # Add buffer
        max_h = (H // 4) + 10
        max_w = (W // 4) + 10
        return max_t * max_h * max_w

    def _compute_feature_shape(self, input_shape: Tuple[int, int, int, int]) -> Tuple[int, int, int, int]:
        """
        Compute feature map shape by running a dummy tensor through the conv layers.
        TorchScript compatible version.
        """
        C, T, H, W = input_shape
        
        with torch.no_grad():
            # Create dummy input
            dummy = torch.zeros(1, C, T, H, W)
            
            # Run through conv layers in order
            x = self.conv_in(dummy)
            x = self.bn_in(x)
            x = self.activation(x)
            
            x = self.layer1(x)
            x = self.layer2(x)
            x = self.global_attention(x)
            x = self.layer3(x)
            x = self.layer4(x)
            
            # Return shape without batch dimension
            return (int(x.shape[1]), int(x.shape[2]), int(x.shape[3]), int(x.shape[4]))

    def forward(self, x: torch.Tensor, debug: bool = False) -> torch.Tensor:
        """
        x: expected shape (batch_size, 2, T, H, W)
        """
        # Initial conv
        if debug: print(f"Input shape: {x.shape}")
        x = self.conv_in(x)
        x = self.bn_in(x)
        x = self.activation(x)
        if debug: print(f"After conv_in: {x.shape}")

        # Residual blocks
        x = self.layer1(x, debug=debug)
        if debug: print(f"After layer1: {x.shape}")
        x = self.layer2(x, debug=debug)
        if debug: print(f"After layer2: {x.shape}")

        # Global attention with skip connection
        x = self.global_attention(x, debug=debug)
        if debug: print(f"After global attention: {x.shape}")
        
        x = self.layer3(x, debug=debug)
        if debug: print(f"After layer3: {x.shape}")
        x = self.layer4(x, debug=debug)
        if debug: print(f"After layer4: {x.shape}")
        
        # Ensure tensor is contiguous before tensor train
        x = x.contiguous()

        # Apply normalized tensor train decomposition
        x = self.tensor_train(x, debug=debug)
        if debug: print(f"After tensor train: {x.shape}")

        # FC layers
        x = self.fc1(x)
        if debug: print(f"After fc1: {x.shape}")
        x = self.activation(x)
        x = self.dropout(x)
        if debug: print(f"After activation and dropout: {x.shape}")
        x = self.fc2(x)
        if debug: print(f"After fc2 (output): {x.shape}")

        return x

    def _initialize_weights(self) -> None:
        """
        Kaiming (He) Initialization for Conv3d and Linear layers.
        """
        for m in self.modules():
            if isinstance(m, nn.Conv3d):
                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)



import torch
import torch.nn as nn
import numpy as np
import time
from typing import Tuple
import math

def create_ultra_complex_3d_dataset(num_samples: int = 2000, 
                                   shape: Tuple[int, int, int, int] = (2, 3, 207, 41),
                                   num_classes: int = 8):
    """
    Creates a challenging synthetic 3D dataset with complex spatiotemporal patterns.
    """
    C, T, H, W = shape
    
    # Create data with distinct patterns for each class
    X = torch.zeros(num_samples, C, T, H, W, dtype=torch.float32)
    y = torch.zeros(num_samples, dtype=torch.long)
    
    samples_per_class = num_samples // num_classes
    
    print(f"ðŸ”¥ Creating ULTRA-COMPLEX dataset with {num_classes} classes...")
    print("   Patterns include: multi-scale waves, spirals, fractals, oscillators, etc.")
    
    for class_idx in range(num_classes):
        start_idx = class_idx * samples_per_class
        end_idx = start_idx + samples_per_class
        
        class_names = ["Waves", "Spirals", "Fractals", "Oscillators", "Flow", "Modulation", "Transforms", "Interference"]
        print(f"   Generating class {class_idx}: {class_names[class_idx]}")
        
        for i in range(start_idx, min(end_idx, num_samples)):
            # Start with structured noise baseline
            sample = torch.randn(C, T, H, W) * 0.15
            
            # Generate patterns for each time step
            for t in range(T):
                if class_idx == 0:  # Multi-scale waves
                    y_coords = torch.linspace(0, 4*math.pi, H).unsqueeze(1)
                    x_coords = torch.linspace(0, 2*math.pi, W).unsqueeze(0)
                    wave1 = torch.sin(2 * y_coords + t * 0.5) * torch.cos(3 * x_coords + t * 0.3)
                    wave2 = torch.sin(5 * y_coords - t * 0.4) * torch.cos(1.5 * x_coords - t * 0.6)
                    sample[0, t] += wave1 + 0.5 * wave2
                    sample[1, t] += 0.7 * wave2 + 0.5 * torch.sin(wave1 + math.pi/4)
                
                elif class_idx == 1:  # Spiral dynamics
                    center_h, center_w = H // 2, W // 2
                    y_grid, x_grid = torch.meshgrid(torch.arange(H), torch.arange(W), indexing='ij')
                    dy = y_grid.float() - center_h
                    dx = x_grid.float() - center_w
                    r = torch.sqrt(dx**2 + dy**2) + 1e-8
                    theta = torch.atan2(dy, dx)
                    spiral = torch.exp(-r / 30) * torch.sin(2 * theta + 0.1 * r + t * 0.4)
                    sample[0, t] += spiral
                    sample[1, t] += torch.roll(spiral, shifts=(t*2, t), dims=(0, 1))
                
                elif class_idx == 2:  # Fractal-like patterns
                    y_grid, x_grid = torch.meshgrid(torch.arange(H), torch.arange(W), indexing='ij')
                    center_h, center_w = H//2, W//2
                    r = torch.sqrt((y_grid.float() - center_h)**2 + (x_grid.float() - center_w)**2) + 1
                    pattern = torch.exp(-r / 50) * torch.sin(r / 5 + t * math.pi) * torch.cos(r / 8 - t * 0.5)
                    sample[0, t] += pattern
                    sample[1, t] += torch.roll(pattern, shifts=(t*3, -t*2), dims=(0, 1))
                
                elif class_idx == 3:  # Oscillating patterns
                    y_coords = torch.linspace(0, 6*math.pi, H).unsqueeze(1)
                    x_coords = torch.linspace(0, 4*math.pi, W).unsqueeze(0)
                    osc1 = torch.sin(y_coords * (1 + t * 0.2)) * torch.cos(x_coords * (2 + t * 0.1))
                    osc2 = torch.cos(y_coords * (0.5 + t * 0.1)) * torch.sin(x_coords * (1.5 + t * 0.15))
                    sample[0, t] += osc1 + 0.5 * osc2
                    sample[1, t] += 0.8 * osc2 + 0.3 * torch.sin(osc1 * 2)
                
                elif class_idx == 4:  # Flow-like patterns
                    y_coords = torch.linspace(0, 2*math.pi, H).unsqueeze(1)
                    x_coords = torch.linspace(0, 2*math.pi, W).unsqueeze(0)
                    flow_y = torch.sin(y_coords + t * 0.3) * torch.cos(x_coords - t * 0.2)
                    flow_x = torch.cos(y_coords - t * 0.4) * torch.sin(x_coords + t * 0.5)
                    sample[0, t] += flow_y
                    sample[1, t] += flow_x
                
                elif class_idx == 5:  # Modulated patterns
                    y_coords = torch.linspace(0, 6*math.pi, H).unsqueeze(1)
                    x_coords = torch.linspace(0, 4*math.pi, W).unsqueeze(0)
                    carrier = torch.sin(2 * y_coords + 3 * x_coords)
                    modulator = 1 + 0.5 * torch.sin(0.5 * y_coords + t * math.pi)
                    modulated = carrier * modulator
                    sample[0, t] += modulated
                    sample[1, t] += torch.sin(modulated + t * 0.5)
                
                elif class_idx == 6:  # Geometric transforms
                    y_coords = torch.linspace(-1, 1, H).unsqueeze(1)
                    x_coords = torch.linspace(-1, 1, W).unsqueeze(0)
                    angle = t * math.pi / 3
                    cos_a, sin_a = math.cos(angle), math.sin(angle)
                    
                    # Rotated coordinates
                    y_rot = y_coords * cos_a - x_coords * sin_a
                    x_rot = y_coords * sin_a + x_coords * cos_a
                    
                    pattern = torch.exp(-(y_rot**2 + x_rot**2) / 0.5) * torch.sin(4 * math.pi * y_rot)
                    sample[0, t] += pattern
                    sample[1, t] += torch.roll(pattern, shifts=(t, -t), dims=(0, 1))
                
                else:  # class 7 - Interference patterns
                    y_grid, x_grid = torch.meshgrid(torch.arange(H), torch.arange(W), indexing='ij')
                    
                    # Wave sources
                    source1_y, source1_x = H//4, W//4
                    source2_y, source2_x = 3*H//4, 3*W//4
                    
                    # Distance from sources
                    d1 = torch.sqrt((y_grid.float() - source1_y)**2 + (x_grid.float() - source1_x)**2) + 1
                    d2 = torch.sqrt((y_grid.float() - source2_y)**2 + (x_grid.float() - source2_x)**2) + 1
                    
                    # Interference waves
                    wave1 = torch.sin(0.5 * d1 + t * 0.8) / d1
                    wave2 = torch.sin(0.3 * d2 - t * 0.6) / d2
                    
                    pattern = wave1 + wave2
                    sample[0, t] += pattern
                    sample[1, t] += pattern * 0.8
            
            # Add structured noise that varies by class
            noise_pattern = torch.randn_like(sample) * (0.1 + 0.05 * class_idx / num_classes)
            sample += noise_pattern
            
            # Normalize to reasonable range
            sample = torch.tanh(sample)
            
            X[i] = sample
            y[i] = class_idx
    
    # Shuffle the dataset
    perm = torch.randperm(num_samples)
    X = X[perm]
    y = y[perm]
    
    print(f"âœ… Created ultra-complex dataset: {X.shape}, {num_classes} classes")
    print(f"   Each sample contains multi-scale, temporal, and geometric patterns")
    
    return X, y

def ultra_challenging_performance_test(model, device, num_epochs: int = 25):
    """
    Ultra-challenging performance test with complex 3D spatiotemporal patterns.
    """
    print("ðŸ”¥ Creating ULTRA-CHALLENGING 3D dataset...")
    print("   This tests multi-scale patterns, temporal dynamics, and geometric flows")
    
    # Create challenging dataset
    train_X, train_y = create_ultra_complex_3d_dataset(1600, shape=(2, 3, 207, 41), num_classes=8)
    test_X, test_y = create_ultra_complex_3d_dataset(400, shape=(2, 3, 207, 41), num_classes=8)
    
    train_X, train_y = train_X.to(device), train_y.to(device)
    test_X, test_y = test_X.to(device), test_y.to(device)
    
    print(f"Train set: {train_X.shape}, Test set: {test_X.shape}")
    print(f"Classes: 8 with complex spatiotemporal patterns")
    
    # Setup training with different learning schedule
    optimizer = torch.optim.Adam(model.parameters(), lr=0.002, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Label smoothing for harder task
    
    batch_size = 12  # Slightly larger batch for stability
    model.train()
    
    print(f"\nðŸš€ Training on ULTRA-COMPLEX patterns for {num_epochs} epochs...")
    print("   Patterns: Waves, Spirals, Fractals, Oscillators, Flow, Modulation, Transforms, Interference")
    
    best_test_acc = 0.0
    
    for epoch in range(num_epochs):
        total_loss = 0.0
        correct = 0
        total = 0
        
        # Mini-batch training
        for i in range(0, len(train_X), batch_size):
            batch_X = train_X[i:i+batch_size]
            batch_y = train_y[i:i+batch_size]
            
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            
            # Gradient clipping for stability
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
        
        scheduler.step()  # Learning rate scheduling
        
        train_acc = 100 * correct / total
        avg_loss = total_loss / (len(train_X) // batch_size)
        
        # Test evaluation
        model.eval()
        with torch.no_grad():
            test_outputs = model(test_X)
            _, test_predicted = torch.max(test_outputs.data, 1)
            test_accuracy = 100 * (test_predicted == test_y).sum().item() / len(test_y)
            
            # Per-class accuracy for detailed analysis
            per_class_acc = []
            for class_idx in range(8):
                class_mask = (test_y == class_idx)
                if class_mask.sum() > 0:
                    class_correct = (test_predicted[class_mask] == test_y[class_mask]).sum().item()
                    class_acc = 100 * class_correct / class_mask.sum().item()
                    per_class_acc.append(class_acc)
                else:
                    per_class_acc.append(0.0)
        
        model.train()
        
        if test_accuracy > best_test_acc:
            best_test_acc = test_accuracy
        
        lr = scheduler.get_last_lr()[0]
        print(f"Epoch {epoch+1:2d}: Loss={avg_loss:.4f}, Train={train_acc:.1f}%, Test={test_accuracy:.1f}%, LR={lr:.5f}")
        
        # Show per-class accuracy every 5 epochs
        if (epoch + 1) % 5 == 0:
            class_names = ["Waves", "Spirals", "Fractals", "Oscillators", "Flow", "Modulation", "Transforms", "Interference"]
            print(f"         Per-class: " + " | ".join([f"{name}:{acc:.0f}%" for name, acc in zip(class_names, per_class_acc)]))
    
    print(f"\nðŸŽ¯ Best Test Accuracy: {best_test_acc:.1f}%")
    print(f"ðŸ“Š ULTRA-CHALLENGE Target: >70% would be exceptional")
    print(f"ðŸ“Š Target: >60% would be very good, >50% would be decent")
    
    return best_test_acc

# Example usage with your model:
if __name__ == "__main__":
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # Your model 
    model = PetNetImproved3D(
        num_classes=8,
        tt_rank=42,
        norm_type=0,
        input_shape=(2, 3, 207, 41)
    ).to(device)
    
    print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # Ultra-challenging test
    final_accuracy = ultra_challenging_performance_test(model, device, num_epochs=25)
    
    print(f"\nðŸ”¥ ULTRA-CHALLENGE RESULT: {final_accuracy:.1f}%")
    if final_accuracy > 70:
        print("ðŸ† EXCEPTIONAL! Your architecture handles ultra-complex 3D patterns!")
    elif final_accuracy > 60:
        print("ðŸ¥‡ EXCELLENT! Very strong performance on challenging patterns!")
    elif final_accuracy > 50:
        print("ðŸ¥ˆ GOOD! Decent performance on very difficult task!")
    else:
        print("ðŸ¥‰ This is genuinely difficult - even 40%+ shows the model is learning!")
    
    # Speed benchmark on complex data
    print(f"\nâš¡ Speed Benchmark on Complex Data:")
    model.eval()
    dummy_input = torch.randn(8, 2, 3, 207, 41).to(device)
    
    # Warmup
    for _ in range(10):
        _ = model(dummy_input)
    
    torch.cuda.synchronize() if device.type == 'cuda' else None
    start_time = time.time()
    
    for _ in range(100):
        with torch.no_grad():
            _ = model(dummy_input)
    
    torch.cuda.synchronize() if device.type == 'cuda' else None
    end_time = time.time()
    
    avg_inference_time = (end_time - start_time) / 100
    print(f"Complex pattern inference (batch=8): {avg_inference_time*1000:.2f}ms")
    print(f"Throughput: {8/avg_inference_time:.1f} samples/sec")