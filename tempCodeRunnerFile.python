import torch
import torch.nn as nn
import time

###############################################################################
# PetNetImproved3D - Your Original Model
###############################################################################

class GlobalAttention3D(nn.Module):
    """
    TorchScript-compatible global attention module for 3D feature maps.
    Uses manual attention implementation instead of nn.MultiheadAttention.
    """
    
    def __init__(self, in_channels=512, embed_dim=128, output_dim=16, num_heads=2):
        super(GlobalAttention3D, self).__init__()
        
        self.in_channels = in_channels
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        
        assert embed_dim % num_heads == 0, "embed_dim must be divisible by num_heads"
        
        # Map from input channels to embedding dimension
        self.channel_proj = nn.Linear(in_channels, embed_dim)
        
        # Manual attention projections (instead of nn.MultiheadAttention)
        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False)
        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)
        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False)
        self.out_proj = nn.Linear(embed_dim, embed_dim)
        
        # Map from embedding dimension to output channels
        self.output_proj = nn.Linear(embed_dim, output_dim)
        
        # Learnable positional encoding will be initialized based on input size
        self.pos_encoding = None
        self.scale = self.head_dim ** -0.5
        
    def _init_positional_encoding(self, D, H, W):
        """Initialize learnable positional encoding for DxHxW spatial positions"""
        num_positions = D * H * W
        self.pos_encoding = nn.Parameter(torch.randn(1, num_positions, self.embed_dim))
        nn.init.normal_(self.pos_encoding, std=0.02)
        
    def forward(self, x):
        """
        x: (batch_size, channels, D, H, W)
        Returns: (batch_size, D*H*W*output_dim) -> flattened for FC layers
        """
        batch_size, channels, D, H, W = x.shape
        seq_len = D * H * W
        
        # Initialize positional encoding on first forward pass
        if self.pos_encoding is None:
            self._init_positional_encoding(D, H, W)
            # Move to same device as input
            self.pos_encoding = self.pos_encoding.to(x.device)
        
        # Reshape to treat spatial positions as sequence tokens
        # (batch_size, channels, D, H, W) -> (batch_size, D*H*W, channels)
        x = x.permute(0, 2, 3, 4, 1).contiguous()  # (batch, D, H, W, channels)
        x = x.view(batch_size, seq_len, channels)  # (batch, seq_len, channels)
        
        # Project to embedding dimension
        x = self.channel_proj(x)  # (batch, seq_len, embed_dim)
        
        # Add positional encoding
        x = x + self.pos_encoding  # (batch, seq_len, embed_dim)
        
        # Manual multi-head attention
        # Project to Q, K, V
        q = self.q_proj(x)  # (batch, seq_len, embed_dim)
        k = self.k_proj(x)  # (batch, seq_len, embed_dim)
        v = self.v_proj(x)  # (batch, seq_len, embed_dim)
        
        # Reshape for multi-head attention
        # (batch, seq_len, embed_dim) -> (batch, seq_len, num_heads, head_dim) -> (batch, num_heads, seq_len, head_dim)
        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Scaled dot-product attention
        # (batch, num_heads, seq_len, head_dim) @ (batch, num_heads, head_dim, seq_len) -> (batch, num_heads, seq_len, seq_len)
        attn_weights = torch.matmul(q, k.transpose(-2, -1)) * self.scale
        attn_weights = torch.softmax(attn_weights, dim=-1)
        
        # Apply attention to values
        # (batch, num_heads, seq_len, seq_len) @ (batch, num_heads, seq_len, head_dim) -> (batch, num_heads, seq_len, head_dim)
        attn_output = torch.matmul(attn_weights, v)
        
        # Concatenate heads
        # (batch, num_heads, seq_len, head_dim) -> (batch, seq_len, num_heads, head_dim) -> (batch, seq_len, embed_dim)
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)
        
        # Output projection
        attn_output = self.out_proj(attn_output)  # (batch, seq_len, embed_dim)
        
        # Project to output channels per token
        output = self.output_proj(attn_output)  # (batch, seq_len, output_dim)
        
        return output


class ResidualBlock3D(nn.Module):
    """
    A 3D adaptation of a 2-layer residual block.
    Uses (3,3,3) kernels and a skip connection.
    'stride' can be a tuple (strideD, strideH, strideW).
    Includes post skip-connection processing layer.
    """

    def __init__(self, in_channels, out_channels, stride=(1, 2, 2)):
        super(ResidualBlock3D, self).__init__()

        # First 3D conv (no padding - manually applied in _apply_circular_padding)
        self.conv1 = nn.Conv3d(in_channels, out_channels,
                               kernel_size=3,
                               stride=stride,
                               padding=0,
                               bias=False)
        self.bn1 = nn.BatchNorm3d(out_channels)

        # Second 3D conv (stride=1 here, no padding)
        self.conv2 = nn.Conv3d(out_channels, out_channels,
                               kernel_size=3,
                               stride=1,
                               padding=0,
                               bias=False)
        self.bn2 = nn.BatchNorm3d(out_channels)

        # Post skip-connection processing layer (no padding)
        self.conv_post = nn.Conv3d(out_channels, out_channels,
                                   kernel_size=3,
                                   stride=1,
                                   padding=0,
                                   bias=False)
        self.bn_post = nn.BatchNorm3d(out_channels)

        self.activation = nn.GELU()

        # Shortcut for matching dimensions if channel or stride changes
        self.shortcut = None
        if stride != (1, 1, 1) or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv3d(in_channels, out_channels,
                          kernel_size=1,
                          stride=stride,
                          bias=False),
                nn.BatchNorm3d(out_channels)
            )

    def _apply_circular_padding(self, x):
        """
        Apply circular padding for width axis (circumferential) and regular padding for time and height.
        For 3x3x3 kernel, we need padding=1 on all sides.
        x shape: (batch, channels, time, height, width)
        """
        # By convention, padding tuple is (pad_last_dim, pad_last_dim, pad_2nd_last, pad_2nd_last, ...)
        # We want: width=circular(1,1), height=constant(1,1), time=constant(1,1)
        
        # First circular padding only to width dimension
        x = torch.nn.functional.pad(x, (1, 1, 0, 0, 0, 0), mode='circular')
        # Then constant padding to time and height dimensions, other dimensions remain unchanged
        x = torch.nn.functional.pad(x, (0, 0, 1, 1, 1, 1), mode='constant', value=0)

        return x

    def forward(self, x):
        # Main path
        out = self._apply_circular_padding(x)
        out = self.conv1(out)
        out = self.bn1(out)
        out = self.activation(out)

        out = self._apply_circular_padding(out)
        out = self.conv2(out)
        out = self.bn2(out)

        # Shortcut
        if self.shortcut is not None:
            x = self.shortcut(x)

        # Residual connection
        out += x

        # Post skip-connection processing
        out = self._apply_circular_padding(out)
        out = self.conv_post(out)
        out = self.bn_post(out)
        out = self.activation(out)
        
        return out


###############################################################################
# Feature Attention Module for FC layers
###############################################################################
class FeatureAttention(nn.Module):
    """
    Attention module for feature vectors before final classification.
    Applies self-attention across feature dimensions.
    """
    
    def __init__(self, feature_dim, embed_dim=512, num_heads=8):
        super(FeatureAttention, self).__init__()
        
        self.feature_dim = feature_dim
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        
        assert embed_dim % num_heads == 0, "embed_dim must be divisible by num_heads"
        
        # Project features to embedding space
        self.feature_proj = nn.Linear(feature_dim, embed_dim)
        
        # Attention projections
        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False)
        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)
        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False)
        self.out_proj = nn.Linear(embed_dim, embed_dim)
        
        # Output projection back to feature space
        self.output_proj = nn.Linear(embed_dim, feature_dim)
        
        self.scale = self.head_dim ** -0.5
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, x):
        """
        x: (batch_size, feature_dim)
        Returns: (batch_size, feature_dim) - attended features
        """
        batch_size, feature_dim = x.shape
        
        # Add sequence dimension for attention (treat as single token)
        x = x.unsqueeze(1)  # (batch, 1, feature_dim)
        
        # Project to embedding space
        x_embed = self.feature_proj(x)  # (batch, 1, embed_dim)
        
        # Generate Q, K, V
        q = self.q_proj(x_embed)  # (batch, 1, embed_dim)
        k = self.k_proj(x_embed)  # (batch, 1, embed_dim)
        v = self.v_proj(x_embed)  # (batch, 1, embed_dim)
        
        # Reshape for multi-head attention
        q = q.view(batch_size, 1, self.num_heads, self.head_dim).transpose(1, 2)
        k = k.view(batch_size, 1, self.num_heads, self.head_dim).transpose(1, 2)
        v = v.view(batch_size, 1, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Self-attention (though with single token, this becomes feature transformation)
        attn_weights = torch.matmul(q, k.transpose(-2, -1)) * self.scale
        attn_weights = torch.softmax(attn_weights, dim=-1)
        attn_weights = self.dropout(attn_weights)
        
        attn_output = torch.matmul(attn_weights, v)
        
        # Concatenate heads
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, 1, self.embed_dim)
        
        # Output projection
        attn_output = self.out_proj(attn_output)  # (batch, 1, embed_dim)
        
        # Project back to feature space and remove sequence dimension
        output = self.output_proj(attn_output).squeeze(1)  # (batch, feature_dim)
        
        # Residual connection
        return output + x.squeeze(1)


class PetNetImproved3D(nn.Module):
    """
    A 3D version of PetNet:
     - Expects input of shape (batch_size, 2, T, 496, 84)
       i.e. 2 "channels" (inner, outer), T frames, 496 Height x 84 Width spatial.
       Width refers to the circumferencial axis. 
     - Uses 3D residual blocks.
     - Ends with global attention.
     - Output => 'num_classes' coordinates or labels.
    """

    def __init__(self, num_classes=6):
        print("Loading PetnetImproved3D Model...")
        super(PetNetImproved3D, self).__init__()

        # Initial 3D conv: 2 => 16 channels
        # kernel_size=3 => (3,3,3)
        # stride=(1,1,1) so we do not reduce T,H,W here
        self.conv_in = nn.Conv3d(in_channels=2, out_channels=16,
                                 kernel_size=3, stride=(1, 1, 1),
                                 padding=1, bias=False)
        self.bn_in = nn.BatchNorm3d(16)
        self.activation = nn.GELU()

        # Residual blocks
        # We apply stride=(1,2,2) to reduce only in the spatial dims (H,W)
        # If you'd like to also reduce T, set strideD>1.
        self.layer1 = ResidualBlock3D(16, 32, stride=(1, 2, 2))  # downsample H,W
        self.layer2 = ResidualBlock3D(32, 64, stride=(1, 2, 2))  # downsample
        self.layer3 = ResidualBlock3D(64, 128, stride=(1, 2, 2))  # downsample
        self.layer4 = ResidualBlock3D(128, 256, stride=(1, 2, 2))  # downsample
        self.layer5 = ResidualBlock3D(256, 512, stride=(1, 2, 2))  # downsample

        # Global attention - Remove the spatial_dims parameter
        self.global_attention = GlobalAttention3D(
            in_channels=512, 
            embed_dim=256, 
            output_dim=16, 
            num_heads=16
        )

        self.dropout = nn.Dropout(0.3)

        # We'll compute fc_in_features dynamically
        fc_in_features = self._compute_fc_input_size()
        self.fc1 = nn.Linear(fc_in_features, 1024, bias=True)
        
        # Add feature attention before final layer
        self.feature_attention = FeatureAttention(
            feature_dim=1024,
            embed_dim=512,
            num_heads=8
        )
        
        self.fc2 = nn.Linear(1024, num_classes, bias=True)

        self._initialize_weights()

    def _compute_fc_input_size(self, C=2, T=3, H=207, W=41):
        """
        Runs a dummy forward pass with shape (1, 2, T, H, W)
        to determine final flattened size going into the FC layer.

        If T is variable in real data, you might pick a "typical" T
        or the max T you expect. The final global pooling removes T anyway,
        but intermediate layers can reduce T if you used stride>1 in depth.
        """
        with torch.no_grad():
            dummy = torch.zeros(1, C, T, H, W)  # (N=1, C=2, D=T, H=496, W=84)
            out = self.conv_in(dummy)
            out = self.bn_in(out)
            out = self.activation(out)

            out = self.layer1(out)
            out = self.layer2(out)
            out = self.layer3(out)
            out = self.layer4(out)
            out = self.layer5(out)

            out = self.global_attention(out)  # => shape (1, seq_len, output_dim)
            out = out.view(1, -1)
            return out.shape[1]

    def forward(self, x, debug=False):
        """
        x: expected shape (batch_size, 2, T, 496, 84)
           (i.e. 2 channels, T frames, 496x84 spatial)
        """
        # initial conv
        if debug: print(f"{x.shape} Input shape")
        x = self.conv_in(x)  # => (B,16,T,496,84)
        if debug: print(f"{x.shape} After conv_in")
        x = self.bn_in(x)
        if debug: print(f"{x.shape} After bn_input")
        x = self.activation(x)
        if debug: print(f"{x.shape} After activation")

        # residual blocks
        x = self.layer1(x)  # => (B,32,T, 496/2=248, 84/2=42), etc.
        if debug: print(f"{x.shape} After layer 1")
        x = self.layer2(x)
        if debug: print(f"{x.shape} After layer 2")
        x = self.layer3(x)
        if debug: print(f"{x.shape} After layer 3")
        x = self.layer4(x)
        if debug: print(f"{x.shape} After layer 4")
        x = self.layer5(x)
        if debug: print(f"{x.shape} After layer 5")

        # global attention => (B, seq_len, output_dim)
        x = self.global_attention(x)
        if debug: print(f"{x.shape} After global attention")

        # flatten => (B, seq_len * output_dim)
        x = x.view(x.size(0), -1)
        if debug: print(f"{x.shape} After flattening")

        # First FC layer
        x = self.fc1(x)
        if debug: print(f"{x.shape} After fc layer 1")
        x = self.activation(x)
        x = self.dropout(x)
        if debug: print(f"{x.shape} After activation and dropout")
        
        # Feature attention module
        x = self.feature_attention(x)
        if debug: print(f"{x.shape} After feature attention")
        
        # Final classification layer
        x = self.fc2(x)
        if debug: print(f"{x.shape} After fc layer 2 (output)")

        return x

    def _initialize_weights(self):
        """
        Kaiming (He) Initialization for Conv3d and Linear layers.
        """
        for m in self.modules():
            if isinstance(m, nn.Conv3d):
                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

###############################################################################
# 3D ResNet-18 BasicBlock (adapted from torchvision)
###############################################################################

class BasicBlock3D(nn.Module):
    """
    Basic building block for ResNet-18 adapted to 3D.
    Maintains the standard ResNet-18 architecture but with 3D convolutions.
    """
    expansion = 1

    def __init__(self, in_channels, out_channels, stride=(1, 1, 1), downsample=None):
        super(BasicBlock3D, self).__init__()
        
        # Both self.conv1 and self.downsample layers downsample the input when stride != 1
        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, 
                               stride=stride, padding=0, bias=False)  # No padding, will be applied manually
        self.bn1 = nn.BatchNorm3d(out_channels)
        
        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3,
                               stride=(1, 1, 1), padding=0, bias=False)  # No padding
        self.bn2 = nn.BatchNorm3d(out_channels)
        
        self.activation = nn.ReLU(inplace=True)
        self.downsample = downsample

    def _apply_circular_padding(self, x):
        """
        Apply circular padding for width axis (circumferential) and regular padding for time and height.
        For 3x3x3 kernel, we need padding=1 on all sides.
        x shape: (batch, channels, time, height, width)
        """
        # Circular padding for width dimension, constant padding for time and height
        x = torch.nn.functional.pad(x, (1, 1, 0, 0, 0, 0), mode='circular')
        x = torch.nn.functional.pad(x, (0, 0, 1, 1, 1, 1), mode='constant', value=0)
        return x

    def forward(self, x):
        identity = x

        # First conv with circular padding
        out = self._apply_circular_padding(x)
        out = self.conv1(out)
        out = self.bn1(out)
        out = self.activation(out)

        # Second conv with circular padding
        out = self._apply_circular_padding(out)
        out = self.conv2(out)
        out = self.bn2(out)

        # Handle downsample for skip connection
        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.activation(out)

        return out


###############################################################################
# 3D ResNet-18 Main Architecture
###############################################################################

class ResNet18_3D(nn.Module):
    """
    ResNet-18 adapted for 3D input data matching PetNetImproved3D specifications.
    
    Standard ResNet-18 layer configuration:
    - conv1: 64 filters
    - layer1: 2 BasicBlocks, 64 filters
    - layer2: 2 BasicBlocks, 128 filters, stride=2
    - layer3: 2 BasicBlocks, 256 filters, stride=2  
    - layer4: 2 BasicBlocks, 512 filters, stride=2
    - avgpool + fc
    
    Adapted for input shape (batch, 2, T, 496, 84) -> output 6 classes
    """

    def __init__(self, num_classes=6):
        print("Loading ResNet18-3D Model...")
        super(ResNet18_3D, self).__init__()
        
        # Initial convolution - matches your conv_in layer concept
        self.conv1 = nn.Conv3d(2, 64, kernel_size=7, stride=(1, 2, 2), 
                               padding=0, bias=False)  # Manual padding
        self.bn1 = nn.BatchNorm3d(64)
        self.relu = nn.ReLU(inplace=True)
        
        # Max pooling (optional - can be disabled for comparison)
        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=(1, 2, 2), padding=0)
        
        # ResNet layers - standard ResNet-18 configuration
        self.layer1 = self._make_layer(BasicBlock3D, 64, 64, 2, stride=(1, 1, 1))
        self.layer2 = self._make_layer(BasicBlock3D, 64, 128, 2, stride=(1, 2, 2))
        self.layer3 = self._make_layer(BasicBlock3D, 128, 256, 2, stride=(1, 2, 2))
        self.layer4 = self._make_layer(BasicBlock3D, 256, 512, 2, stride=(1, 2, 2))
        
        # Global average pooling instead of attention (standard ResNet approach)
        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))
        
        # Classifier
        self.dropout = nn.Dropout(0.3)
        self.fc = nn.Linear(512 * BasicBlock3D.expansion, num_classes)
        
        self._initialize_weights()

    def _apply_circular_padding_7x7(self, x):
        """Circular padding for 7x7 kernel (padding=3)"""
        x = torch.nn.functional.pad(x, (3, 3, 0, 0, 0, 0), mode='circular')
        x = torch.nn.functional.pad(x, (0, 0, 3, 3, 3, 3), mode='constant', value=0)
        return x
    
    def _apply_circular_padding_3x3(self, x):
        """Circular padding for 3x3 kernel (padding=1)"""
        x = torch.nn.functional.pad(x, (1, 1, 0, 0, 0, 0), mode='circular')
        x = torch.nn.functional.pad(x, (0, 0, 1, 1, 1, 1), mode='constant', value=0)
        return x

    def _make_layer(self, block, in_channels, out_channels, blocks, stride=(1, 1, 1)):
        """Create a layer with multiple blocks"""
        downsample = None
        if stride != (1, 1, 1) or in_channels != out_channels * block.expansion:
            downsample = nn.Sequential(
                nn.Conv3d(in_channels, out_channels * block.expansion,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm3d(out_channels * block.expansion),
            )

        layers = []
        layers.append(block(in_channels, out_channels, stride, downsample))
        
        for _ in range(1, blocks):
            layers.append(block(out_channels * block.expansion, out_channels))

        return nn.Sequential(*layers)

    def forward(self, x, debug=False):
        """
        Forward pass with optional debug prints for shape tracking
        x: expected shape (batch_size, 2, T, 496, 84)
        """
        if debug: print(f"{x.shape} Input shape")
        
        # Initial convolution with circular padding
        x = self._apply_circular_padding_7x7(x)
        x = self.conv1(x)
        if debug: print(f"{x.shape} After conv1")
        x = self.bn1(x)
        x = self.relu(x)
        if debug: print(f"{x.shape} After bn1 + relu")
        
        # Max pooling with circular padding
        x = self._apply_circular_padding_3x3(x)
        x = self.maxpool(x)
        if debug: print(f"{x.shape} After maxpool")
        
        # ResNet layers
        x = self.layer1(x)
        if debug: print(f"{x.shape} After layer1")
        x = self.layer2(x)
        if debug: print(f"{x.shape} After layer2")
        x = self.layer3(x)
        if debug: print(f"{x.shape} After layer3")
        x = self.layer4(x)
        if debug: print(f"{x.shape} After layer4")
        
        # Global average pooling
        x = self.avgpool(x)
        if debug: print(f"{x.shape} After avgpool")
        
        # Flatten and classify
        x = torch.flatten(x, 1)
        if debug: print(f"{x.shape} After flatten")
        x = self.dropout(x)
        x = self.fc(x)
        if debug: print(f"{x.shape} After fc (output)")
        
        return x

    def _initialize_weights(self):
        """Initialize weights using Kaiming initialization"""
        for m in self.modules():
            if isinstance(m, nn.Conv3d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm3d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                nn.init.constant_(m.bias, 0)


###############################################################################
# Alternative: ResNet-18 WITHOUT MaxPooling (closer to PetNet structure)
###############################################################################

class ResNet18_3D_NoMaxPool(ResNet18_3D):
    """
    ResNet-18 variant without max pooling to more closely match 
    PetNetImproved3D's progressive downsampling approach
    """
    
    def __init__(self, num_classes=6):
        super().__init__(num_classes)
        print("Loading ResNet18-3D (No MaxPool) Model...")
        
        # Remove maxpool by making it identity
        self.maxpool = nn.Identity()
        
        # Adjust first conv to compensate for removed maxpool
        self.conv1 = nn.Conv3d(2, 64, kernel_size=3, stride=(1, 2, 2), 
                               padding=0, bias=False)

    def forward(self, x, debug=False):
        """Forward pass without maxpooling"""
        if debug: print(f"{x.shape} Input shape")
        
        # Initial convolution 
        x = self._apply_circular_padding_3x3(x)
        x = self.conv1(x)
        if debug: print(f"{x.shape} After conv1")
        x = self.bn1(x)
        x = self.relu(x)
        if debug: print(f"{x.shape} After bn1 + relu")
        
        # Skip maxpool (identity operation)
        x = self.maxpool(x)
        if debug: print(f"{x.shape} After maxpool (identity)")
        
        # ResNet layers
        x = self.layer1(x)
        if debug: print(f"{x.shape} After layer1")
        x = self.layer2(x)
        if debug: print(f"{x.shape} After layer2")
        x = self.layer3(x)
        if debug: print(f"{x.shape} After layer3")
        x = self.layer4(x)
        if debug: print(f"{x.shape} After layer4")
        
        # Global average pooling
        x = self.avgpool(x)
        if debug: print(f"{x.shape} After avgpool")
        
        # Flatten and classify
        x = torch.flatten(x, 1)
        if debug: print(f"{x.shape} After flatten")
        x = self.dropout(x)
        x = self.fc(x)
        if debug: print(f"{x.shape} After fc (output)")
        
        return x


###############################################################################
# Performance Comparison Test with Training Metrics
###############################################################################

def train_and_evaluate_model(model, model_name, device, epochs=50):
    """Train a model and return performance metrics"""
    
    # Test parameters
    B, C, T, H, W = 8, 2, 3, 207, 41
    CLASSES = 6
    
    # Generate consistent data for fair comparison
    torch.manual_seed(42)
    dummy_input = torch.randn(B, C, T, H, W).to(device)
    dummy_target = torch.randn(B, CLASSES).to(device)
    
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # Training metrics
    training_times = []
    losses = []
    
    print(f"\nTraining {model_name}...")
    
    for epoch in range(epochs):
        start_time = time.time()
        
        optimizer.zero_grad()
        output = model(dummy_input)
        loss = criterion(output, dummy_target)
        loss.backward()
        optimizer.step()
        
        end_time = time.time()
        epoch_time = end_time - start_time
        
        training_times.append(epoch_time)
        losses.append(loss.item())
        
        if epoch % 10 == 0 or epoch == epochs - 1:
            print(f"  Epoch {epoch+1:3d}/{epochs}, Loss: {loss.item():.6f}, Time: {epoch_time:.4f}s")
    
    # Calculate metrics
    avg_epoch_time = sum(training_times) / len(training_times)
    final_loss = losses[-1]
    loss_reduction = losses[0] - final_loss
    convergence_speed = sum(1 for i in range(1, len(losses)) if abs(losses[i] - losses[i-1]) < 0.001)
    
    return {
        'model_name': model_name,
        'avg_epoch_time': avg_epoch_time,
        'final_loss': final_loss,
        'initial_loss': losses[0],
        'loss_reduction': loss_reduction,
        'loss_history': losses,
        'convergence_epochs': convergence_speed
    }


def compare_models():
    """Compare all three models with comprehensive metrics"""
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    # Test parameters
    B, C, T, H, W = 8, 2, 3, 207, 41
    CLASSES = 6
    
    # Create models
    models = {
        'PetNetImproved3D': PetNetImproved3D(num_classes=CLASSES).to(device),
        'ResNet18_3D': ResNet18_3D(num_classes=CLASSES).to(device),
        'ResNet18_3D_NoMaxPool': ResNet18_3D_NoMaxPool(num_classes=CLASSES).to(device)
    }
    
    # Model analysis
    model_stats = {}
    
    print("="*70)
    print("MODEL ARCHITECTURE ANALYSIS")
    print("="*70)
    
    for name, model in models.items():
        param_count = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        # Memory usage estimation (rough)
        model_size_mb = param_count * 4 / (1024 * 1024)  # 4 bytes per float32
        
        model_stats[name] = {
            'parameters': param_count,
            'trainable_parameters': trainable_params,
            'model_size_mb': model_size_mb
        }
        
        print(f"\n{name}:")
        print(f"  Total parameters: {param_count:,}")
        print(f"  Trainable parameters: {trainable_params:,}")
        print(f"  Estimated model size: {model_size_mb:.2f} MB")
    
    # Test forward pass shapes
    print("\n" + "="*70)
    print("FORWARD PASS VERIFICATION")
    print("="*70)
    
    torch.manual_seed(42)
    dummy_input = torch.randn(B, C, T, H, W).to(device)
    
    for name, model in models.items():
        print(f"\n{name} Forward Pass:")
        try:
            with torch.no_grad():
                start_time = time.time()
                output = model(dummy_input, debug=False)
                inference_time = time.time() - start_time
            
            model_stats[name]['inference_time'] = inference_time
            model_stats[name]['output_shape'] = output.shape
            print(f"  ✓ Success! Output shape: {output.shape}")
            print(f"  ✓ Inference time: {inference_time:.4f}s")
        except Exception as e:
            print(f"  ✗ Error: {e}")
            model_stats[name]['error'] = str(e)
    
    # Training comparison
    print("\n" + "="*70)
    print("TRAINING PERFORMANCE COMPARISON")
    print("="*70)
    
    training_results = {}
    
    for name, model in models.items():
        if 'error' not in model_stats[name]:
            result = train_and_evaluate_model(model, name, device, epochs=300)
            training_results[name] = result
            model_stats[name].update(result)
    
    # Final summary
    print("\n" + "="*70)
    print("COMPREHENSIVE MODEL COMPARISON SUMMARY")
    print("="*70)
    
    # Create comparison table
    print(f"\n{'Metric':<25} {'PetNetImproved3D':<20} {'ResNet18_3D':<20} {'ResNet18_NoMaxPool':<20}")
    print("-" * 85)
    
    # Architecture metrics
    print(f"{'Parameters':<25} {model_stats['PetNetImproved3D']['parameters']:>18,} {model_stats['ResNet18_3D']['parameters']:>18,} {model_stats['ResNet18_3D_NoMaxPool']['parameters']:>18,}")
    print(f"{'Model Size (MB)':<25} {model_stats['PetNetImproved3D']['model_size_mb']:>18.2f} {model_stats['ResNet18_3D']['model_size_mb']:>18.2f} {model_stats['ResNet18_3D_NoMaxPool']['model_size_mb']:>18.2f}")
    
    # Performance metrics (if training completed)
    if all(name in training_results for name in models.keys()):
        print(f"{'Inference Time (s)':<25} {model_stats['PetNetImproved3D']['inference_time']:>18.4f} {model_stats['ResNet18_3D']['inference_time']:>18.4f} {model_stats['ResNet18_3D_NoMaxPool']['inference_time']:>18.4f}")
        print(f"{'Avg Epoch Time (s)':<25} {model_stats['PetNetImproved3D']['avg_epoch_time']:>18.4f} {model_stats['ResNet18_3D']['avg_epoch_time']:>18.4f} {model_stats['ResNet18_3D_NoMaxPool']['avg_epoch_time']:>18.4f}")
        print(f"{'Final Loss':<25} {model_stats['PetNetImproved3D']['final_loss']:>18.6f} {model_stats['ResNet18_3D']['final_loss']:>18.6f} {model_stats['ResNet18_3D_NoMaxPool']['final_loss']:>18.6f}")
        print(f"{'Loss Reduction':<25} {model_stats['PetNetImproved3D']['loss_reduction']:>18.6f} {model_stats['ResNet18_3D']['loss_reduction']:>18.6f} {model_stats['ResNet18_3D_NoMaxPool']['loss_reduction']:>18.6f}")
    
    # Architecture-specific insights
    print(f"\n{'ARCHITECTURE INSIGHTS':<50}")
    print("-" * 50)
    
    print(f"PetNetImproved3D:")
    print(f"  • Custom residual blocks with post-skip processing")
    print(f"  • Global attention mechanism with positional encoding")
    print(f"  • 5-layer progressive downsampling (16→512 channels)")
    print(f"  • GELU activation throughout")
    
    print(f"\nResNet18_3D:")
    print(f"  • Standard ResNet-18 architecture adapted to 3D")
    print(f"  • Global average pooling for feature aggregation")
    print(f"  • 4-layer structure with maxpooling")
    print(f"  • ReLU activation throughout")
    
    print(f"\nResNet18_3D_NoMaxPool:")
    print(f"  • ResNet-18 without maxpooling layer")
    print(f"  • More similar downsampling pattern to PetNet")
    print(f"  • Simpler architecture, potentially faster training")
    
    # Recommendations
    print(f"\n{'RECOMMENDATIONS':<50}")
    print("-" * 50)
    
    if all(name in training_results for name in models.keys()):
        fastest_training = min(training_results.items(), key=lambda x: x[1]['avg_epoch_time'])
        best_convergence = min(training_results.items(), key=lambda x: x[1]['final_loss'])
        smallest_model = min(model_stats.items(), key=lambda x: x[1]['parameters'])
        
        print(f"• Fastest Training: {fastest_training[0]} ({fastest_training[1]['avg_epoch_time']:.4f}s/epoch)")
        print(f"• Best Convergence: {best_convergence[0]} (final loss: {best_convergence[1]['final_loss']:.6f})")
        print(f"• Smallest Model: {smallest_model[0]} ({smallest_model[1]['parameters']:,} parameters)")
        
        print(f"\nFor your use case:")
        print(f"• If training speed is critical → Choose ResNet18_3D_NoMaxPool")
        print(f"• If model expressiveness is critical → Choose PetNetImproved3D")
        print(f"• If memory is constrained → Choose {smallest_model[0]}")
    
    return model_stats, training_results


if __name__ == "__main__":
    model_stats, training_results = compare_models()